{
"bugs": [
{
"date_closed": "2012-01-18T00:06:14.785796+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Critical", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-01-17T01:50:10.587466+00:00", 
"bug_id": 917265, 
"date_in_progress": "2012-01-18T00:06:14.785796+00:00", 
"bug_description": "I'm trying to setup a Mysql cluster with 2 nodes, following this link: http://www.percona.com/doc/percona-xtradb-cluster/3nodesec2.html\n\nNode 1 starts normal.\nNode 2 fails to start.  I found this in his mysql error log:\n\n\n120116 16:49:11 [Warning] WSREP: Gap in state sequence. Need state transfer.\n120116 16:49:13 [Note] WSREP: Running: 'wsrep_sst_rsync 'joiner' '10.59.13.185' '' '/var/lib/mysql/' '/etc/mysql/my.cnf' '18321' 2>sst.err'\n120116 16:49:13 [ERROR] WSREP: Failed to read 'ready <addr>' from: wsrep_sst_rsync 'joiner' '10.59.13.185' '' '/var/lib/mysql/' '/etc/mysql/my.cnf' '18321' 2>sst.err\n        Read: '(null)'\n120116 16:49:13 [ERROR] WSREP: Process completed with error: wsrep_sst_rsync 'joiner' '10.59.13.185' '' '/var/lib/mysql/' '/etc/mysql/my.cnf' '18321' 2>sst.err: 2 (No such file or directory)\n120116 16:49:13 [ERROR] WSREP: Failed to prepare for 'rsync' SST. Unrecoverable.\n120116 16:49:13 [ERROR] Aborting\n\n\nThe file  wsrep_sst_rsync does not exist on my system (Amazon Ubuntu Lucid 10.04.3).  I have installed everything on this site:\nhttp://www.percona.com/downloads/Percona-XtraDB-Cluster/5.5.17-alpha22.1/deb/lucid/\n\nand XtraBackup:\nhttp://www.percona.com/downloads/XtraBackup/XtraBackup-1.6.4/deb/lucid/x86_64/", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/917265/related_tasks", 
"date_assigned": "2012-01-17T01:50:32.063782+00:00", 
"bug_information_type": "Public", 
"bug_title": "wsrep_sst_rsync: no such file or directory", 
"title": "Bug #917265 in Percona XtraDB Cluster: \"wsrep_sst_rsync: no such file or directory\"", 
"date_confirmed": "2012-01-17T01:50:10.587466+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/917265", 
"date_left_new": "2012-01-17T01:50:10.587466+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/917265", 
"milestone_link": null, 
"http_etag": "\"74b3a5d316d24b14ff441bf8b450f17edf636f64-4fd4a934787c17c61b2c2f135133df3b33d86c06\"", 
"owner_link": "https://api.launchpad.net/devel/~kzqdnsw4i2-peter-x8oxkp4um1", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-01-18T00:06:14.785796+00:00", 
"date_fix_released": "2012-01-18T00:06:14.785796+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/917265", 
"date_created": "2012-01-16T17:21:34.330018+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"pkg"
]
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_triaged": null, 
"bug_id": 917837, 
"date_in_progress": null, 
"bug_description": "We need to execute the cluster test suites against our packages.\nWriting this bug / assigning to myself to track the issue.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/917837/related_tasks", 
"date_assigned": "2012-01-17T19:51:02.191859+00:00", 
"bug_information_type": "Public", 
"bug_title": "Need cluster packaging tests", 
"title": "Bug #917837 in Percona XtraDB Cluster: \"Need cluster packaging tests\"", 
"date_confirmed": "2012-01-17T19:51:05.316498+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/917837", 
"date_left_new": "2012-01-17T19:51:05.316498+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/917837", 
"milestone_link": null, 
"http_etag": "\"352b0e5a9257f5a86a85f2a5a65f9d6a4ed6059a-8ba5ab9f32b49557a7638f8f31f1b78d9e0d22d3\"", 
"owner_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/917837", 
"date_created": "2012-01-17T19:50:49.434810+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-03-20T04:11:36.366587+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-03-20T04:11:36.366587+00:00", 
"bug_id": 940472, 
"date_in_progress": "2012-03-20T04:11:36.366587+00:00", 
"bug_description": "I have downloaded the RHEL5 Source RPM: Percona-XtraDB-Cluster-5.5.20-beta23.4.3724.rhel5.src.rpm .\n\nAttempting to build with the stock spec file produces all the packages except Percona-XtraDB-Cluster-galera which is required for installation.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/940472/related_tasks", 
"date_assigned": "2012-02-24T18:01:58.862061+00:00", 
"bug_information_type": "Public", 
"bug_title": "[BUILD]  RHEL5 Source RPM will not build Percona-XtraDB-Cluster-galera package", 
"title": "Bug #940472 in Percona XtraDB Cluster: \"[BUILD]  RHEL5 Source RPM will not build Percona-XtraDB-Cluster-galera package\"", 
"date_confirmed": "2012-02-24T18:01:53.898783+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/940472", 
"date_left_new": "2012-02-24T18:01:53.898783+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/940472", 
"milestone_link": null, 
"http_etag": "\"42a92257841e0ec34356e00e33901b52597b567c-f743a6f04ba68a45e8bac1521856c7b6ec668d11\"", 
"owner_link": "https://api.launchpad.net/devel/~ammon-sutherland", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-03-20T04:11:36.366587+00:00", 
"date_fix_released": "2012-03-20T04:11:36.366587+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/940472", 
"date_created": "2012-02-24T17:30:44.398081+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": null, 
"status": "Fix Committed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-03-03T00:22:22.491519+00:00", 
"bug_id": 944166, 
"date_in_progress": "2012-03-03T00:22:22.491519+00:00", 
"bug_description": "XtraDB-Cluster-server packages (RPM and DEB) should have xtrabackup in dependencies, and we have that.\nBut we should have specific version i.e. 1.9.0+, because older versions will not work.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/944166/related_tasks", 
"date_assigned": "2012-03-01T17:36:45.413654+00:00", 
"bug_information_type": "Public", 
"bug_title": "XtraDB-Cluster-server package dependency", 
"title": "Bug #944166 in Percona XtraDB Cluster: \"XtraDB-Cluster-server package dependency\"", 
"date_confirmed": "2012-03-01T17:36:54.355989+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/944166", 
"date_left_new": "2012-03-01T17:36:54.355989+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/944166", 
"milestone_link": null, 
"http_etag": "\"f985aefb233ab535026764a812c3ade3d83d5f00-81ee69eebda1f7a3d5dee02ee431b486cc346d50\"", 
"owner_link": "https://api.launchpad.net/devel/~vadim-tk", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-03-03T00:22:22.491519+00:00", 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/944166", 
"date_created": "2012-03-01T17:36:16.829132+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"pkg"
]
}, 
{
"date_closed": "2012-05-02T19:47:28.290712+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-03-07T21:02:59.573263+00:00", 
"bug_id": 947410, 
"date_in_progress": "2012-03-07T21:02:59.573263+00:00", 
"bug_description": "[In:How to setup 3 node cluster in EC2 enviroment]\n\nUsed the same AMI: \tami-250e5060\n\nbut its version 6.2 now and doing the basic yum install it failed:\n\nyum install Percona-XtraDB-Cluster-server Percona-XtraDB-Cluster-client\n\n\n--> Finished Dependency Resolution\nError: Percona-XtraDB-Cluster-shared conflicts with Percona-Server-shared-51\n You could try using --skip-broken to work around the problem\n You could try running: rpm -Va --nofiles --nodigest", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/947410/related_tasks", 
"date_assigned": "2012-03-07T21:03:35.513805+00:00", 
"bug_information_type": "Public", 
"bug_title": "Percona-XtraDB-Cluster-shared conflicts with Percona-Server-shared-51", 
"title": "Bug #947410 in Percona XtraDB Cluster: \"Percona-XtraDB-Cluster-shared conflicts with Percona-Server-shared-51\"", 
"date_confirmed": "2012-03-07T21:02:25.001361+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/947410", 
"date_left_new": "2012-03-07T21:02:25.001361+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/947410", 
"milestone_link": null, 
"http_etag": "\"ada9e65b5f7fe3a89ed1b3dbe08f8ffb2fcb40c7-f743a6f04ba68a45e8bac1521856c7b6ec668d11\"", 
"owner_link": "https://api.launchpad.net/devel/~mikerapuano", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-05-02T19:47:28.290712+00:00", 
"date_fix_released": "2012-05-02T19:47:28.290712+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/947410", 
"date_created": "2012-03-05T19:39:04.243052+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc", 
"pkg"
]
}, 
{
"date_closed": null, 
"status": "Fix Committed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-03-20T20:04:46.178279+00:00", 
"bug_id": 959970, 
"date_in_progress": "2012-03-20T20:04:46.178279+00:00", 
"bug_description": "When using the xtrabackup SST on a Debian Squeeze based system, the SST will error out with an error similar to the below:\n--\n120320  2:19:18 [ERROR] WSREP: Process completed with error: wsrep_sst_xtrabackup 'joiner' '192.168.0.2' '' '/var/lib/mysql/' '/etc/mysql/my.cnf' '31891' 2>sst.err: 2 (No such file or directory)\n--\n\nsst.err contains the actual error message:\n--\n~$# cat /var/lib/mysql/sst.err \nnc: invalid option -- 'd'\nnc -h for help\ntar: This does not look like a tar archive\ntar: Exiting with failure status due to previous errors\n--\n\nA quick look at the man page for nc on Debian Squeeze shows the -d flag does not exist.\n\n(I've removed -d from the SST script, but the transfer still doesn't seem to want to work. Have not investigated why yet)", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/959970/related_tasks", 
"date_assigned": "2012-03-20T17:17:07.972021+00:00", 
"bug_information_type": "Public", 
"bug_title": "xtrabackup SST netcat \"-d\" option not available on Debian Squeeze", 
"title": "Bug #959970 in Percona XtraDB Cluster: \"xtrabackup SST netcat \"-d\" option not available on Debian Squeeze\"", 
"date_confirmed": "2012-03-20T20:04:46.178279+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/959970", 
"date_left_new": "2012-03-20T20:04:46.178279+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/959970", 
"milestone_link": null, 
"http_etag": "\"f3c0fed759d120c373a46c37542afb20a35c1bc0-81ee69eebda1f7a3d5dee02ee431b486cc346d50\"", 
"owner_link": "https://api.launchpad.net/devel/~chris.boulton", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-03-20T20:04:46.178279+00:00", 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/959970", 
"date_created": "2012-03-20T07:31:51.488950+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 960569, 
"date_in_progress": null, 
"bug_description": "As explained in bug #959970, the xtrabackup SST script depends on the -d option of netcat, which for debian and ubuntu is only available in the non-default netcat-openbsd package.\n\nIt's desirable to have it work with the version of netcat provided by the netcat-traditional package.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/960569/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Make xtrabackup SST script work with any netcat", 
"title": "Bug #960569 in Percona XtraDB Cluster: \"Make xtrabackup SST script work with any netcat\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/960569", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/960569", 
"milestone_link": null, 
"http_etag": "\"c67a5b4112f8517cc8aae35788b201b7ed4642ba-0f5d4381ee8da83235a916df1bc483f6fe7fd824\"", 
"owner_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/960569", 
"date_created": "2012-03-20T20:03:59.665345+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-06-12T21:11:06.216039+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-06-12T21:11:06.216039+00:00", 
"bug_id": 999495, 
"date_in_progress": "2012-06-12T21:11:06.216039+00:00", 
"bug_description": "The binaries avialble here have tarballs that are missing the '5' from '23.5'\n\nhttp://www.percona.com/downloads/Percona-XtraDB-Cluster/5.5.23-23.5/binary/linux/\n\nexample\n\nhttp://www.percona.com/redir/downloads/Percona-XtraDB-Cluster/5.5.23-23.5/binary/linux/x86_64/Percona-XtraDB-Cluster-5.5.23-23..333.Linux.x86_64.tar.gz\n\nAlso, I'd expect a hyphen between the release and the revision instead of a <period>\n\nThanks!", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/999495/related_tasks", 
"date_assigned": "2012-06-06T14:46:18.336249+00:00", 
"bug_information_type": "Public", 
"bug_title": "tarballs missing part of version number", 
"title": "Bug #999495 in Percona XtraDB Cluster: \"tarballs missing part of version number\"", 
"date_confirmed": "2012-06-12T21:11:06.216039+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/999495", 
"date_left_new": "2012-06-12T21:11:06.216039+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/999495", 
"milestone_link": null, 
"http_etag": "\"0d65247a70aa7dc03724ff496cef314db0818afa-f743a6f04ba68a45e8bac1521856c7b6ec668d11\"", 
"owner_link": "https://api.launchpad.net/devel/~travisghansen", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-06-12T21:11:06.216039+00:00", 
"date_fix_released": "2012-06-12T21:11:06.216039+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/999495", 
"date_created": "2012-05-15T06:55:01.404093+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": null, 
"bug_id": 1000761, 
"date_in_progress": null, 
"bug_description": "[root@percona1 log]# which clustercheck\n/usr/bin/clustercheck\n[root@percona1 log]# cat /etc/xinetd.d/mysqlchk \n# default: on \n# description: mysqlchk \nservice mysqlchk \n{ \n# this is a config for xinetd, place it in /etc/xinetd.d/\n        disable = no \n        flags           = REUSE \n        socket_type     = stream \n        port            = 9200 \n        wait            = no \n        user            = nobody \n        server          = /usr/local/bin/clustercheck\n        log_on_failure  += USERID \n        only_from       = 0.0.0.0/0 \n        # recommended to put the IPs that need \n        # to connect exclusively (security purposes) \n        per_source      = UNLIMITED \n}", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1000761/related_tasks", 
"date_assigned": "2012-09-20T18:20:50.747556+00:00", 
"bug_information_type": "Public", 
"bug_title": "mysqlchk xinetd config incorrectly refers to /usr/local/bin/clustercheck instead of /usr/bin/clustercheck", 
"title": "Bug #1000761 in Percona XtraDB Cluster: \"mysqlchk xinetd config incorrectly refers to /usr/local/bin/clustercheck instead of /usr/bin/clustercheck\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1000761", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1000761", 
"milestone_link": null, 
"http_etag": "\"33fdad70f409c5df54c632c0c5ba856cb322e88c-fa820b328c6bb0c4302db3eaae894c7630640e7d\"", 
"owner_link": "https://api.launchpad.net/devel/~jay-janssen", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1000761", 
"date_created": "2012-05-17T13:57:27.751654+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Fix Committed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "High", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": "2012-06-25T05:44:38.002444+00:00", 
"bug_id": 1008961, 
"date_in_progress": "2012-08-23T18:47:28.571791+00:00", 
"bug_description": "The  percona-xtradb-cluster-galera-2.x package should conflict with \"galera\" in the \"Conflicts:\" part. \n\nElse it breaks the complete installation and leaves a pretty nasty mess in your system. I had to give it several install/remove/reinstall/purge tries until it was running smoothly.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1008961/related_tasks", 
"date_assigned": "2012-06-05T13:49:13.688952+00:00", 
"bug_information_type": "Public", 
"bug_title": "Missing conflict for  percona-xtradb-cluster-galera-2.x", 
"title": "Bug #1008961 in Percona XtraDB Cluster: \"Missing conflict for  percona-xtradb-cluster-galera-2.x\"", 
"date_confirmed": "2012-06-12T23:15:12.763085+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1008961", 
"date_left_new": "2012-06-12T23:15:12.763085+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1008961", 
"milestone_link": null, 
"http_etag": "\"57b4e0123244bd2278168b7bbecc62536c61919b-81ee69eebda1f7a3d5dee02ee431b486cc346d50\"", 
"owner_link": "https://api.launchpad.net/devel/~maik-kulbe", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-08-23T18:47:28.571791+00:00", 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1008961", 
"date_created": "2012-06-05T11:43:17.181937+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"pkg"
]
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Medium", 
"assignee_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_triaged": null, 
"bug_id": 618603, 
"date_in_progress": null, 
"bug_description": "To reproduce, create a simple view:\n\nCREATE VIEW v (mycol) AS SELECT 'abc';\n\nThen, check the view status in 'master' and 'slave' nodes:\n\nmaster:\n\nmysql> show create view v\\G\n*************************** 1. row ***************************\n                View: v\n         Create View: CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`bulldog` SQL SECURITY DEFINER VIEW `v` AS select `abc` AS `mycol`\n...\n\nslave:\nmysql> show create view v\\G\n*************************** 1. row ***************************\n                View: v\n         Create View: CREATE ALGORITHM=UNDEFINED DEFINER=``@`` SQL SECURITY DEFINER VIEW `v` AS select `abc` AS `mycol`\n...", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/618603/related_tasks", 
"date_assigned": "2012-02-01T21:36:39.247419+00:00", 
"bug_information_type": "Public", 
"bug_title": "view definer is not replicated", 
"title": "Bug #618603 in Percona XtraDB Cluster: \"view definer is not replicated\"", 
"date_confirmed": "2012-02-01T21:36:58.648573+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/618603", 
"date_left_new": "2012-02-01T21:36:58.648573+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/618603", 
"milestone_link": null, 
"http_etag": "\"87b862828351bc5c84ec24a6bd7e7a1afb5617f9-65f1dc87bc92be781eaee36b48244ad3c179c976\"", 
"owner_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/618603", 
"date_created": "2012-02-01T21:36:27.296725+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Medium", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": null, 
"bug_id": 966679, 
"date_in_progress": null, 
"bug_description": "As per the Percona FAQ:\nhttp://www.percona.com/doc/percona-xtradb-cluster/faq.html#q-how-can-i-check-the-galera-node-health\n\u201cUnknown error (node is online but Galera is not connected/synced with the cluster)\u201d\n\n1)\tWsrep_OSU_method=RSU on node1/2/3\n\n2)\t Node1/2/3:  mysql \u2013u root \u2013p \u2013e\u2019show create table t1\u2019\n+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                                                                 |\n+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t1    | CREATE TABLE `t1` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `hostname` varchar(64) NOT NULL,\n  `port` int(11) NOT NULL,\n  `instime` datetime NOT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=51875647 DEFAULT CHARSET=latin1 |\n+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\n3)\t Beginning row counts of t1 on all three nodes:\n+----------+----------------------------+---------------------+\n| count(*) | @@hostname                 | now()               |\n+----------+----------------------------+---------------------+\n| 17291893 | Node1 | 2012-03-26 10:45:31 |\n+----------+----------------------------+---------------------+\n+----------+----------------------------+---------------------+\n| count(*) | @@hostname                 | now()               |\n+----------+----------------------------+---------------------+\n| 17291893 | node2 | 2012-03-26 10:45:32 |\n+---------+----------+----------------------------+---------------------+\n+---------+----------+----------------------------+---------------------+\n | count(*) | @@hostname                 | now()               |\n+---------+----------+----------------------------+---------------------+\n| 17291893 | node3 | 2012-03-26 10:45:31 |\n+---------+----------+----------------------------+---------------------+\n\n\n4)\tAdd a column to table t1 on node1:\n\nmysql> select now();\n+---------------------+\n| now()               |\n+---------------------+\n| 2012-03-26 10:46:15 |\n+---------------------+\n1 row in set (0.00 sec)\n\nmysql> ALTER TABLE repl.t1 add column10 int;\nQuery OK, 17291893 rows affected (2 min 55.97 sec)\nRecords: 17291893  Duplicates: 0  Warnings: 0\n\nmysql> select now();\n+---------------------+\n| now()               |\n+---------------------+\n| 2012-03-26 10:49:11 |\n+---------------------+\n1 row in set (0.00 sec)\n\n\n\n5)\tLogin to node1 -  able to select a row from the table under-going RSU (node1) - was expecting to get an \"Unknown Command\" message based on the FAQ linked above.\n\nmysql> select *,now() from t1 where port=9999;  \uf0df \n+----------+----------+------+---------------------+---------------------+\n| id       | hostname | port | instime             | now()               |\n+----------+----------+------+---------------------+---------------------+\n| 51875649 | RSU-test | 9999 | 2012-03-26 10:44:19 | 2012-03-26 10:46:57 |\n+----------+----------+------+---------------------+---------------------+\n1 row in set (7.04 sec)\n\n\n6)\t Log messages from all three nodes:\n\n[root@node1]# cat node1.err\n120326 10:46:15 [Note] WSREP: Node 1 (node1) desyncs itself from group\n120326 10:46:15 [Note] WSREP: Shifting SYNCED -> DONOR/DESYNCED (TO: 28807712)\n120326 10:46:15 [Note] WSREP: Provider paused at ea8d8c3e-5bba-11e1-0800-0bdfe5c74f3a:28807712\n120326 10:49:11 [Note] WSREP: Provider resumed.\n120326 10:49:11 [Note] WSREP: Node 1 (node1) resyncs itself to group\n120326 10:49:11 [Note] WSREP: Shifting DONOR/DESYNCED -> JOINED (TO: 28807712)\n120326 10:49:11 [Note] WSREP: Member 1 (node1) synced with group.\n120326 10:49:11 [Note] WSREP: Shifting JOINED -> SYNCED (TO: 28807712)\n120326 10:49:11 [Note] WSREP: Synchronized with group, ready for connections\n\n[root@node2]# cat node2.err\n120326 10:46:16 [Note] WSREP: Node 1 (node1) desyncs itself from group\n120326 10:49:12 [Note] WSREP: Node 1 (node1) resyncs itself to group\n120326 10:49:12 [Note] WSREP: Member 1 (node1) synced with group.\n\n[root@node3]# cat node3.err\n120326 10:46:14 [Note] WSREP: Node 1 (node1) desyncs itself from group\n120326 10:49:10 [Note] WSREP: Node 1 (node1) resyncs itself to group\n120326 10:49:10 [Note] WSREP: Member 1 (node1) synced with group.\n\n\n\n7)\tshow create table t1\n\nnode1 (node that underwent RSU):\nCREATE TABLE `t1` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `hostname` varchar(64) NOT NULL,\n  `port` int(11) NOT NULL,\n  `instime` datetime NOT NULL,\n  `column10` int(11) DEFAULT NULL,  \uf0df New column has been added.\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=51875652 DEFAULT CHARSET=latin1 \n\nnode2\nCREATE TABLE `t1` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `hostname` varchar(64) NOT NULL,\n  `port` int(11) NOT NULL,\n  `instime` datetime NOT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=51875650 DEFAULT CHARSET=latin1 \n\nnode3\nCREATE TABLE `t1` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `hostname` varchar(64) NOT NULL,\n  `port` int(11) NOT NULL,\n  `instime` datetime NOT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=51875650 DEFAULT CHARSET=latin1 \n\n\nThanks\nPatrick", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/966679/related_tasks", 
"date_assigned": "2012-03-28T15:46:25.748383+00:00", 
"bug_information_type": "Public", 
"bug_title": "Able to query table under-going an RSU while node is desyned from Cluster", 
"title": "Bug #966679 in Percona XtraDB Cluster: \"Able to query table under-going an RSU while node is desyned from Cluster\"", 
"date_confirmed": "2012-03-28T15:46:34.188102+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/966679", 
"date_left_new": "2012-03-28T15:46:34.188102+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/966679", 
"milestone_link": null, 
"http_etag": "\"d69fb2187e680f2e3f49d4651a16f1a8dbe33cea-1d203c4eec997519c1d08281c2a8c371a5eb296e\"", 
"owner_link": "https://api.launchpad.net/devel/~patrickzoblisein", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/966679", 
"date_created": "2012-03-27T22:45:53.822980+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-04-19T20:11:47.752682+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Medium", 
"assignee_link": null, 
"date_triaged": "2012-04-19T20:11:47.752682+00:00", 
"bug_id": 982003, 
"date_in_progress": "2012-04-19T20:11:47.752682+00:00", 
"bug_description": "I'm not really sure whether this is my fault, missing documentation, or a real bug. but i can't seem to found setting for xtrabackup_sst authentication.\n\nI use percona-xtradb-cluster 5.5 from percona's debian repository. Initially, I am having the exact same error as the original report on bug #975794. sst.err gave me \"xtrabackup-2.0: This does not look like a tar archive\", except i have datadir on my config.\n\nso i check innobackup.backup.log on donor. and there it is this message:\n\n120415 07:14:42  innobackupex: Starting mysql with options:  --unbuffered --\n120415 07:14:42  innobackupex: Connected to database with mysql child process (pid=8541)\ninnobackupex: Error: mysql child process has died: ERROR 1045 (28000): Access denied for user 'mysql'@'localhost' (using password: NO)\n\nIt seems xtrabackup sst invoke innobackupex using mysql user without any auth seting. Is there some way to change this? changing wsrep_sst_auth doesn't help (and your doc says it's not necessary). \n\nCurrently the only solution I know is to manually edit wsrep_sst_xtrabackup to include user/password flags, and risk got wiped on upgrade. or create mysql@localhost user without password. Would be nicer if there is something better.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/982003/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "need authentication setting for xtrabackup sst", 
"title": "Bug #982003 in Percona XtraDB Cluster: \"need authentication setting for xtrabackup sst\"", 
"date_confirmed": "2012-04-17T05:41:53.956742+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/982003", 
"date_left_new": "2012-04-17T05:41:53.956742+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/982003", 
"milestone_link": null, 
"http_etag": "\"db2552cd9c09cb415764bac3a54f42cece044bbd-6527cfbb51376d93445f252f32cca5ece0123611\"", 
"owner_link": "https://api.launchpad.net/devel/~j-ahmad", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-04-19T20:11:47.752682+00:00", 
"date_fix_released": "2012-04-19T20:11:47.752682+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/982003", 
"date_created": "2012-04-15T00:32:18.837443+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": "2012-06-27T11:17:15.676997+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Medium", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-06-27T08:43:31.716589+00:00", 
"bug_id": 1000214, 
"date_in_progress": "2012-06-27T08:43:31.716589+00:00", 
"bug_description": "I believe the issue is that the xtradb mysqld can't understand the wsrep_urls variable.\n\n\n120516 15:00:04 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql\n120516 15:00:04 [Note] Flashcache bypass: disabled\n120516 15:00:04 [Note] Flashcache setup error is : ioctl failed\n\n120516 15:00:04 [Note] WSREP: Read nil XID from storage engines, skipping position init\n120516 15:00:04 [Note] WSREP: wsrep_load(): loading provider library '/usr/lib64/libgalera_smm.so'\n120516 15:00:04 [Note] WSREP: wsrep_load(): Galera 2.1dev(r112) by Codership Oy <email address hidden> loaded succesfully.\n120516 15:00:04 [Note] WSREP: Found saved state: 00000000-0000-0000-0000-000000000000:-1\n120516 15:00:04 [Note] WSREP: Reusing existing '/var/lib/mysql//galera.cache'.\n120516 15:00:04 [Note] WSREP: Passing config to GCS: base_host = 10.0.2.15; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n120516 15:00:04 [Note] WSREP: Assign initial position for certification: -1, protocol version: -1\n120516 15:00:04 [Note] WSREP: wsrep_sst_grab()\n120516 15:00:04 [Note] WSREP: Start replication\n120516 15:00:04 [Note] WSREP: Setting initial position to 00000000-0000-0000-0000-000000000000:-1\n120516 15:00:04 [Note] WSREP: protonet asio version 0\n120516 15:00:04 [Note] WSREP: backend: asio\n120516 15:00:04 [Note] WSREP: GMCast version 0\n120516 15:00:04 [Note] WSREP: (0e4a60e5-9f57-11e1-0800-b7eac9697238, 'tcp://0.0.0.0:4567') listening at tcp://0.0.0.0:4567\n120516 15:00:04 [Note] WSREP: (0e4a60e5-9f57-11e1-0800-b7eac9697238, 'tcp://0.0.0.0:4567') multicast: , ttl: 1\n120516 15:00:04 [Note] WSREP: EVS version 0\n120516 15:00:04 [Note] WSREP: PC version 0\n120516 15:00:04 [Note] WSREP: gcomm: connecting to group 'trimethylxanthine', peer '192.168.70.2:'\n120516 15:00:04 [Note] WSREP: (0e4a60e5-9f57-11e1-0800-b7eac9697238, 'tcp://0.0.0.0:4567') turning message relay requesting on, nonlive peers: tcp://192.168.70.3:4567 \n120516 15:00:05 [Note] WSREP: declaring 1a92d07e-9f56-11e1-0800-f59148046fe4 stable\n120516 15:00:05 [Note] WSREP: declaring bd9cd0f2-99e4-11e1-0800-6806eb833b63 stable\n120516 15:00:05 [Note] WSREP: (0e4a60e5-9f57-11e1-0800-b7eac9697238, 'tcp://0.0.0.0:4567') turning message relay requesting off\n120516 15:00:05 [Note] WSREP: view(view_id(PRIM,0e4a60e5-9f57-11e1-0800-b7eac9697238,10) memb {\n\t0e4a60e5-9f57-11e1-0800-b7eac9697238,\n\t1a92d07e-9f56-11e1-0800-f59148046fe4,\n\tbd9cd0f2-99e4-11e1-0800-6806eb833b63,\n} joined {\n} left {\n} partitioned {\n})\n120516 15:00:05 [Note] WSREP: gcomm: connected\n120516 15:00:05 [Note] WSREP: Changing maximum packet size to 64500, resulting msg size: 32636\n120516 15:00:05 [Note] WSREP: Shifting CLOSED -> OPEN (TO: 0)\n120516 15:00:05 [Note] WSREP: Opened channel 'trimethylxanthine'\n120516 15:00:05 [Note] WSREP: Waiting for SST to complete.\n120516 15:00:05 [Note] WSREP: New COMPONENT: primary = yes, bootstrap = no, my_idx = 0, memb_num = 3\n120516 15:00:05 [Note] WSREP: STATE_EXCHANGE: sent state UUID: 0ee3aa83-9f57-11e1-0800-73682d4a05de\n120516 15:00:05 [Note] WSREP: STATE EXCHANGE: sent state msg: 0ee3aa83-9f57-11e1-0800-73682d4a05de\n120516 15:00:05 [Note] WSREP: STATE EXCHANGE: got state msg: 0ee3aa83-9f57-11e1-0800-73682d4a05de from 0 (percona3)\n120516 15:00:05 [Note] WSREP: STATE EXCHANGE: got state msg: 0ee3aa83-9f57-11e1-0800-73682d4a05de from 1 (percona1)\n120516 15:00:05 [Note] WSREP: STATE EXCHANGE: got state msg: 0ee3aa83-9f57-11e1-0800-73682d4a05de from 2 (percona2)\n120516 15:00:05 [Note] WSREP: Quorum results:\n\tversion    = 2,\n\tcomponent  = PRIMARY,\n\tconf_id    = 7,\n\tmembers    = 2/3 (joined/total),\n\tact_id     = 772,\n\tlast_appl. = -1,\n\tprotocols  = 0/3/1 (gcs/repl/appl),\n\tgroup UUID = c19f6c17-99e2-11e1-0800-861728b65ef4\n120516 15:00:05 [Note] WSREP: Flow-control interval: [14, 28]\n120516 15:00:05 [Note] WSREP: Shifting OPEN -> PRIMARY (TO: 772)\n120516 15:00:05 [Note] WSREP: State transfer required: \n\tGroup state: c19f6c17-99e2-11e1-0800-861728b65ef4:772\n\tLocal state: 00000000-0000-0000-0000-000000000000:-1\n120516 15:00:05 [Note] WSREP: New cluster view: global state: c19f6c17-99e2-11e1-0800-861728b65ef4:772, view# 8: Primary, number of nodes: 3, my index: 0, protocol version 1\n120516 15:00:05 [Warning] WSREP: Gap in state sequence. Need state transfer.\n120516 15:00:07 [Note] WSREP: Running: 'wsrep_sst_xtrabackup 'joiner' '192.168.70.4' '' '/var/lib/mysql/' '/etc/my.cnf' '26692' 2>sst.err'\n120516 15:00:07 [Note] WSREP: Prepared SST request: xtrabackup|192.168.70.4:4444/xtrabackup_sst\n120516 15:00:07 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120516 15:00:07 [Note] WSREP: Assign initial position for certification: 772, protocol version: 2\n120516 15:00:07 [Warning] WSREP: Failed to prepare for incremental state transfer: Local state UUID (00000000-0000-0000-0000-000000000000) does not match group state UUID (c19f6c17-99e2-11e1-0800-861728b65ef4): 1 (Operation not permitted)\n\t at galera/src/replicator_str.cpp:prepare_for_IST():439. IST will be unavailable.\n120516 15:00:07 [Note] WSREP: Node 0 (percona3) requested state transfer from '*any*'. Selected 1 (percona1)(SYNCED) as donor.\n120516 15:00:07 [Note] WSREP: Shifting PRIMARY -> JOINER (TO: 772)\n120516 15:00:07 [Note] WSREP: Requesting state transfer: success, donor: 1\n120516 15:01:44 [Note] WSREP: 1 (percona1): State transfer to 0 (percona3) complete.\n120516 15:01:44 [Note] WSREP: Member 1 (percona1) synced with group.\n120516 15:01:56 [Note] WSREP: SST complete, seqno: 772\n120516 15:01:56 [Warning] No argument was provided to --log-bin, and --log-bin-index was not used; so replication may break when this MySQL server acts as a master and has his hostname changed!! Please use '--log-bin=percona3-bin' to avoid this problem.\n120516 15:01:56 [Note] Plugin 'FEDERATED' is disabled.\n120516 15:01:56 InnoDB: The InnoDB memory heap is disabled\n120516 15:01:56 InnoDB: Mutexes and rw_locks use GCC atomic builtins\n120516 15:01:56 InnoDB: Compressed tables use zlib 1.2.3\n120516 15:01:56 InnoDB: Using Linux native AIO\n120516 15:01:56 InnoDB: Initializing buffer pool, size = 128.0M\n120516 15:01:56 InnoDB: Completed initialization of buffer pool\n120516 15:01:56 InnoDB: highest supported file format is Barracuda.\n120516 15:01:56  InnoDB: Waiting for the background threads to start\n120516 15:01:57 Percona XtraDB (http://www.percona.com) 1.1.8-rel25.3 started; log sequence number 2173783564\n120516 15:01:57 [ERROR] /usr/sbin/mysqld: unknown variable 'wsrep_urls=gcomm://192.168.70.2,gcomm://192.168.70.3,gcomm://'\n120516 15:01:57 [ERROR] Aborting\n\n120516 15:01:59 [Note] WSREP: Closing send monitor...\n120516 15:01:59 [Note] WSREP: Closed send monitor.\n120516 15:01:59 [Note] WSREP: gcomm: terminating thread\n120516 15:01:59 [Note] WSREP: gcomm: joining thread\n120516 15:01:59 [Note] WSREP: gcomm: closing backend\n120516 15:01:59 [Note] WSREP: view(view_id(NON_PRIM,0e4a60e5-9f57-11e1-0800-b7eac9697238,10) memb {\n\t0e4a60e5-9f57-11e1-0800-b7eac9697238,\n} joined {\n} left {\n} partitioned {\n\t1a92d07e-9f56-11e1-0800-f59148046fe4,\n\tbd9cd0f2-99e4-11e1-0800-6806eb833b63,\n})\n120516 15:01:59 [Note] WSREP: New COMPONENT: primary = no, bootstrap = no, my_idx = 0, memb_num = 1\n120516 15:01:59 [Note] WSREP: view((empty))\n120516 15:01:59 [Note] WSREP: gcomm: closed\n120516 15:01:59 [Note] WSREP: Flow-control interval: [8, 16]\n120516 15:01:59 [Note] WSREP: Received NON-PRIMARY.\n120516 15:01:59 [Note] WSREP: Shifting JOINER -> OPEN (TO: 772)\n120516 15:01:59 [Note] WSREP: Received self-leave message.\n120516 15:01:59 [Note] WSREP: Flow-control interval: [0, 0]\n120516 15:01:59 [Note] WSREP: Received SELF-LEAVE. Closing connection.\n120516 15:01:59 [Note] WSREP: Shifting OPEN -> CLOSED (TO: 772)\n120516 15:01:59 [Note] WSREP: RECV thread exiting 0: Success\n120516 15:01:59 [Note] WSREP: recv_thread() joined.\n120516 15:01:59 [Note] WSREP: Closing slave action queue.\n120516 15:01:59 [Note] WSREP: Service disconnected.\n120516 15:01:59 [Note] WSREP: rollbacker thread exiting\n120516 15:02:00 [Note] WSREP: Some threads may fail to exit.\n120516 15:02:00  InnoDB: Starting shutdown...\n120516 15:02:02  InnoDB: Shutdown completed; log sequence number 2173783564\n120516 15:02:02 [Note] /usr/sbin/mysqld: Shutdown complete\n\nError in my_thread_global_end(): 1 threads didn't exit\n120516 15:02:07 mysqld_safe mysqld from pid file /var/lib/mysql/percona3.pid ended\n\n\n\n\n# cat /etc/my.cnf \n[mysqld]\ndatadir=/var/lib/mysql\nuser=mysql\nlog_error=error.log\nbinlog_format=ROW\nwsrep_provider=/usr/lib64/libgalera_smm.so\nwsrep_urls=gcomm://192.168.70.2,gcomm://192.168.70.3,gcomm://\n#wsrep_cluster_address=gcomm://\nwsrep_sst_receive_address=192.168.70.4\nwsrep_node_incoming_address=192.168.70.4 \nwsrep_slave_threads=2\nwsrep_cluster_name=trimethylxanthine\n#wsrep_sst_method=rsync\nwsrep_sst_method=xtrabackup\nwsrep_node_name=percona3\ninnodb_locks_unsafe_for_binlog=1\ninnodb_autoinc_lock_mode=2\ninnodb_log_file_size=64M\nbind-address=192.168.70.4\n\nserver-id=3\nlog-bin\nlog-slave-updates\n\n[mysql]\nuser=root\nprompt=\"percona3 mysql> \"", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1000214/related_tasks", 
"date_assigned": "2012-05-16T13:12:41.849663+00:00", 
"bug_information_type": "Public", 
"bug_title": "cluster node can't sst (xtrabackup) with wsrep_urls in /etc/my.cnf", 
"title": "Bug #1000214 in Percona XtraDB Cluster: \"cluster node can't sst (xtrabackup) with wsrep_urls in /etc/my.cnf\"", 
"date_confirmed": "2012-05-16T13:12:23.990660+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1000214", 
"date_left_new": "2012-05-16T13:12:18.797034+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1000214", 
"milestone_link": null, 
"http_etag": "\"7c880837136eece556ff84cbe0c73aa2610b0ba5-c36eec55d6d1da4c89c3b3db10e57b381f285e0e\"", 
"owner_link": "https://api.launchpad.net/devel/~jay-janssen", 
"date_left_closed": "2012-05-16T13:12:23.990660+00:00", 
"date_incomplete": null, 
"date_fix_committed": "2012-06-27T10:11:09.800431+00:00", 
"date_fix_released": "2012-06-27T11:17:15.676997+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1000214", 
"date_created": "2012-05-16T13:07:20.919933+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-07-30T12:55:48.069563+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Medium", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-07-26T09:42:24.956219+00:00", 
"bug_id": 1029336, 
"date_in_progress": "2012-07-26T10:32:21.631584+00:00", 
"bug_description": "[In:Percona XtraDB Cluster Documentation]\n\nThe 3-node installation walk-throughs in the documentation do not mention what to do with the empty cluster address (wsrep_cluster_address=gcomm://) in the config of the node that was used to bootstrap the cluster.\n\nAs I understand this should not be left empty but should be changed to one of the other nodes (through a set global and persisted in the my.cnf). Not doing so will make node 1 think it has to start a new cluster when you restart it instead of joining the existing one.\n\nI think this should be added to the documentation as a cluster installation is not complete without it.\n\ncheers,\nvanne", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1029336/related_tasks", 
"date_assigned": "2012-07-26T09:42:36.244841+00:00", 
"bug_information_type": "Public", 
"bug_title": "[DOC] Cluster install docs incomplete", 
"title": "Bug #1029336 in Percona XtraDB Cluster: \"[DOC] Cluster install docs incomplete\"", 
"date_confirmed": "2012-07-26T09:42:24.956219+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1029336", 
"date_left_new": "2012-07-26T09:42:24.956219+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1029336", 
"milestone_link": null, 
"http_etag": "\"5403ef9b84db8e06bf8bdd2f3804d76909651584-c36eec55d6d1da4c89c3b3db10e57b381f285e0e\"", 
"owner_link": "https://api.launchpad.net/devel/~vanten", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-07-30T09:37:48.430208+00:00", 
"date_fix_released": "2012-07-30T12:55:48.069563+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1029336", 
"date_created": "2012-07-26T09:30:14.746904+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-08-15T05:04:58.994765+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Medium", 
"assignee_link": null, 
"date_triaged": "2012-08-15T05:04:58.994765+00:00", 
"bug_id": 1036957, 
"date_in_progress": "2012-08-15T05:04:58.994765+00:00", 
"bug_description": "Improved script is reported by Olaf van Zandwijk,\nit is there https://github.com/olafz/percona-clustercheck/", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1036957/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Include a better clustercheck script", 
"title": "Bug #1036957 in Percona XtraDB Cluster: \"Include a better clustercheck script\"", 
"date_confirmed": "2012-08-15T05:04:58.994765+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1036957", 
"date_left_new": "2012-08-15T05:04:58.994765+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1036957", 
"milestone_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+milestone/percona-xtradb-cluster-5.5.27", 
"http_etag": "\"ee59dd439006f6150eb0683aec6e9cfe5d84778c-79388aba33b2d15bcbe62201f4a5723e042bc3b0\"", 
"owner_link": "https://api.launchpad.net/devel/~vadim-tk", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-08-15T05:04:58.994765+00:00", 
"date_fix_released": "2012-08-15T05:04:58.994765+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1036957", 
"date_created": "2012-08-15T05:04:41.319877+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": "2012-01-08T20:45:26.596300+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-01-08T20:45:26.596300+00:00", 
"bug_id": 912963, 
"date_in_progress": "2012-01-08T20:45:26.596300+00:00", 
"bug_description": "From the kewpie cluster test suite:\n20120106-180200 INFO STATUS: FAIL, 16/33 test cases, 48.48 percent executed\n20120106-180200 INFO STATUS: PASS, 17/33 test cases, 51.52 percent executed\n20120106-180200 INFO FAIL tests: cluster_basic.alterAddCol1Neg_test, cluster_basic.alterAddCol1_test, cluster_basic.alterAddColAfter_test, cluster_basic.alterAddIndexNeg_test, cluster_basic.alterAddIndex_test, cluster_basic.alterChangeCol_test, cluster_basic.alterDropColNeg_test, cluster_basic.alterDropFK_test, cluster_basic.alterDropIndexNeg_test, cluster_basic.alterRenameNeg_test, cluster_basic.alterRenamePos_test, cluster_basic.basic_test, cluster_basic.replace1_test, cluster_basic.replaceMultiRow_test, cluster_basic.replaceSelect_test, cluster_basic.replaceSet_test\n20120106-180200 INFO Spent 188 / 788 seconds on: TEST(s)\n\n\nThese are failing in various ways from not replicating the changes to crashing servers, I include one of the more interesting failures below.\n\nTo duplicate:\nbranch kewpie:\nbzr branch kewpie\nprereqs (dbd::mysql perl module, MySQLdb python module, python 2.6)\n./kewpie.py --basedir=/path/to/server/ --default-server-type=galera --suite=cluster_basic --wsrep-provider-path=/path/to/libgalera_smm.so --force\n\nExample failure:\n\t\n*** buffer overflow detected ***: percona-xtradb-cluster/sql/mysqld terminated\n*** buffer overflow detected ***: percona-xtradb-cluster/sql/mysqld terminated\n/lib/libc.so.6(__fortify_fail+0x37)[0x7fb4cd43c217]\n======= Backtrace: =========\n/lib/libc.so.6(+0xfe0d0)[0x7fb4cd43b0d0]\n/lib/libc.so.6(__fortify_fail+0x37)[0x7fcee433f217]\n/lib/libc.so.6(+0xfd539)[0x7fb4cd43a539]\n/lib/libc.so.6(+0xfe0d0)[0x7fcee433e0d0]\n/lib/libc.so.6(_IO_default_xsputn+0xcc)[0x7fb4cd3b2d1c]\n/lib/libc.so.6(+0xfd539)[0x7fcee433d539]\n/lib/libc.so.6(_IO_vfprintf+0x3d34)[0x7fb4cd3860d4]\n/lib/libc.so.6(_IO_default_xsputn+0xcc)[0x7fcee42b5d1c]\n/lib/libc.so.6(__vsprintf_chk+0x99)[0x7fb4cd43a5d9]\n/lib/libc.so.6(_IO_vfprintf+0x3d34)[0x7fcee42890d4]\n/lib/libc.so.6(__sprintf_chk+0x7f)[0x7fb4cd43a51f]\n/lib/libc.so.6(__vsprintf_chk+0x99)[0x7fcee433d5d9]\n/lib/libc.so.6(__sprintf_chk+0x7f)[0x7fcee433d51f]\npercona-xtradb-cluster/sql/mysqld(_Z19wsrep_write_rbr_bufP3THDPKvm+0x75)[0x588445]\npercona-xtradb-cluster/sql/mysqld(_Z19wsrep_write_rbr_bufP3THDPKvm+0x75)[0x588445]\npercona-xtradb-cluster/sql/mysqld(_Z14wsrep_apply_cbPvPKvml+0xe2)[0x595eb2]\npercona-xtradb-cluster/sql/mysqld(_Z14wsrep_apply_cbPvPKvml+0xe2)[0x595eb2]\n/pxc-galera/libgalera_smm.so(+0x1a9f32)[0x7fb4c3e59f32]\n/pxc-galera/libgalera_smm.so(+0x1a9f32)[0x7fcedad5cf32]\n/pxc-galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM9apply_trxEPvPNS_9TrxHandleE+0x67)[0x7fb4c3e5f9f7]\n/pxc-galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM9apply_trxEPvPNS_9TrxHandleE+0x67)[0x7fcedad629f7]\n/pxc-galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM11process_trxEPvPNS_9TrxHandleE+0x45)[0x7fb4c3e60025]\n/pxc-galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM11process_trxEPvPNS_9TrxHandleE+0x45)[0x7fcedad63025]\n/pxc-galera/libgalera_smm.so(_ZN6galera15GcsActionSource8dispatchEPvRK10gcs_action+0x3e1)[0x7fb4c3e3d361]\n/pxc-galera/libgalera_smm.so(_ZN6galera15GcsActionSource8dispatchEPvRK10gcs_action+0x3e1)[0x7fcedad40361]\n/pxc-galera/libgalera_smm.so(_ZN6galera15GcsActionSource7processEPv+0x58)[0x7fb4c3e3db08]\n/pxc-galera/libgalera_smm.so(_ZN6galera15GcsActionSource7processEPv+0x58)[0x7fcedad40b08]\n/pxc-galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM10async_recvEPv+0x105)[0x7fb4c3e5b3a5]\n/pxc-galera/libgalera_smm.so(_ZN6galera13ReplicatorSMM10async_recvEPv+0x105)[0x7fcedad5e3a5]\n/pxc-galera/libgalera_smm.so(galera_recv+0x23)[0x7fb4c3e745f3]\n/pxc-galera/libgalera_smm.so(galera_recv+0x23)[0x7fcedad775f3]\npercona-xtradb-cluster/sql/mysqld(_Z25wsrep_replication_processP3THD+0x112)[0x58ef42]\npercona-xtradb-cluster/sql/mysqld(_Z25wsrep_replication_processP3THD+0x112)[0x58ef42]\npercona-xtradb-cluster/sql/mysqld(start_wsrep_THD+0x4bd)[0x50d1bd]\n/lib/libpthread.so.0(+0x69ca)[0x7fb4ce3a79ca]\npercona-xtradb-cluster/sql/mysqld(start_wsrep_THD+0x4bd)[0x50d1bd]\n/lib/libc.so.6(clone+0x6d)[0x7fb4cd42370d]\n======= Memory map: ========\n/lib/libc.so.6(clone+0x6d)[0x7fcee432670d]\n======= Memory map: ========\n00400000-00d6f000 r-xp 00000000 08:01 26097061                           percona-xtradb-cluster/sql/mysqld\n00f6f000-00f72000 r--p 0096f000 08:01 26097061                           percona-xtradb-cluster/sql/mysqld\n00f72000-01009000 rw-p 00972000 08:01 26097061                           percona-xtradb-cluster/sql/mysqld\n01009000-01039000 rw-p 00000000 00:00 0 \n01f58000-03f9d000 rw-p 00000000 00:00 0                                  [heap]\n7fb4b4000000-7fb4b4044000 rw-p 00000000 00:00 0 \n7fb4b4044000-7fb4b8000000 ---p 00000000 00:00 0 \n7fb4b9574000-7fb4b9575000 ---p 00000000 00:00 0 \n7fb4b9575000-7fb4b95a5000 rw-p 00000000 00:00 0 \n7fb4b95a5000-7fb4b95a6000 ---p 00000000 00:00 0 \n7fb4b95a6000-7fb4b9da6000 rw-p 00000000 00:00 0 \n7fb4b9da6000-7fb4b9da7000 ---p 00000000 00:00 0 \n7fb4b9da7000-7fb4b9dd7000 rw-p 00000000 00:00 0 \n7fb4b9dd7000-7fb4b9dd8000 ---p 00000000 00:00 0 \n7fb4b9dd8000-7fb4b9e08000 rw-p 00000000 00:00 0 \n7fb4b9e08000-7fb4b9e09000 00400000-00d6f000 r-xp 00000000 08:01 26097061                           percona-xtradb-cluster/sql/mysqld\n00f6f000-00f72000 r--p 0096f000 08:01 26097061                           percona-xtradb-cluster/sql/mysqld\n00f72000-01009000 rw-p 00972000 08:01 26097061                           percona-xtradb-cluster/sql/mysqld\n01009000-01039000 rw-p 00000000 00:00 0 \n01b37000-03b7c000 rw-p 00000000 00:00 0                                  [heap]\n7fcecc000000-7fcecc043000 rw-p 00000000 00:00 0 \n7fcecc043000-7fced0000000 ---p 00000000 00:00 0 \n7fced0477000-7fced0478000 ---p 00000000 00:00 0 \n7fced0478000-7fced04a8000 rw-p 00000000 00:00 0 \n7fced04a8000-7fced04a9000 ---p 00000000 00:00 0 \n7fced04a9000-7fced0ca9000 rw-p 00000000 00:00 0 \n7fced0ca9000-7fced0caa000 ---p 00000000 00:00 0 \n7fced0caa000-7fced0cda000 rw-p 00000000 00:00 0 \n7fced0cda000-7fced0cdb000 ---p 00000000 00:00 0 \n7fced0cdb000-7fced0d0b000 rw-p 00000000 00:00 0 \n7fced0d0b000-7fced0d0c000 ---p 00000000 00:00 0 \n7fced0d0c000-7fced150c000 rw-p 00000000 00:00 0 \n7fced150c000-7fced150d000 ---p 00000000 00:00 0 \n7fced150d000-7fced1d0d000 rw-p 00000000 00:00 0 \n7fced1d0d000-7fced1d0e000 ---p 00000000 00:00 0 \n7fced1d0e000-7fced250e000 rw-p 00000000 00:00 0 \n7fced250e000-7fceda50f000 rw-s 00000000 00:10 8088739                    /dev/shm/qp_workdir_pcrews_b99fa507-4887-42d8-922f-2da1c0144815/bot0/var_s2/master-data/galera.cache\n7fceda50f000-7fceda677000 r-xp 00000000 08:01 262222                     /lib/libcrypto.so.0.9.8\n7fceda677000-7fceda876000 ---p 00168000 08:01 262222                     /lib/libcrypto.so.0.9.8\n7fceda876000-7fceda883000 r--p 00167000 08:01 262222                     /lib/libcrypto.so.0.9.8\n7fceda883000-7fceda89b000 rw-p 00174000 08:01 262222                     /lib/libcrypto.so.0.9.8\n7fceda89b000-7fceda89f000 rw-p 00000000 00:00 0 \n7fceda89f000-7fceda995000 r-xp 00000000 08:01 28315311                   /usr/lib/libstdc++.so.6.0.13\n7fceda995000-7fcedab95000 ---p 000f6000 08:01 28315311                   /usr/lib/libstdc++.so.6.0.13\n7fcedab95000-7fcedab9c000 r--p 000f6000 08:01 28315311                   /usr/lib/libstdc++.so.6.0.13\n7fcedab9c000-7fcedab9e000 rw-p 000fd000 08:01 28315311                   /usr/lib/libstdc++.so.6.0.13\n7fcedab9e000-7fcedabb3000 rw-p 00000000 00:00 0 \n7fcedabb3000-7fcedadca000 r-xp 00000000 08:01 26351125                   /pxc-galera/libgalera_smm.so\n7fcedadca000-7fcedafc9000 ---p 00217000 08:01 26351125                   /pxc-galera/libgalera_smm.so\n7fcedafc9000-7fcedafd0000 r--p 00216000 08:01 26351125                   /pxc-galera/libgalera_smm.so20120106-174328  ===============================================================\n20120106-174328  TEST NAME                                  [ RESULT ] TIME (ms)\n20120106-174328  ===============================================================\n20120106-174328  cluster_basic.alterAddCol1Neg_test         [ fail ]    10445\n20120106-174328  test_alterAddCol_neg (alterAddCol1Neg_test.basicTest) ... ERROR", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/912963/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Many basic replication test cases failing with latest galera liba", 
"title": "Bug #912963 in Percona XtraDB Cluster: \"Many basic replication test cases failing with latest galera liba\"", 
"date_confirmed": "2012-01-08T20:45:26.596300+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/912963", 
"date_left_new": "2012-01-08T20:45:26.596300+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/912963", 
"milestone_link": null, 
"http_etag": "\"c911d93907a2b2b0020dbf56f167666eafa8b7f4-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-01-08T20:45:26.596300+00:00", 
"date_fix_released": "2012-01-08T20:45:26.596300+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/912963", 
"date_created": "2012-01-06T23:19:06.645060+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": "2012-01-11T17:52:33.540729+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-01-11T17:52:33.540729+00:00", 
"bug_id": 914906, 
"date_in_progress": "2012-01-11T17:52:33.540729+00:00", 
"bug_description": "[In:Glossary]\n\ns/Percon XtraDB Cluster/Percona XtraDB Cluster/\n\nThere's two occurrences on that page.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/914906/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "[DOC] percona-xtradb-cluster/glossary.html", 
"title": "Bug #914906 in Percona XtraDB Cluster: \"[DOC] percona-xtradb-cluster/glossary.html\"", 
"date_confirmed": "2012-01-11T17:52:33.540729+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/914906", 
"date_left_new": "2012-01-11T17:52:33.540729+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/914906", 
"milestone_link": null, 
"http_etag": "\"0bc996b5cb1910f3c3b202740cf057c576a01241-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~morgo", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-01-11T17:52:33.540729+00:00", 
"date_fix_released": "2012-01-11T17:52:33.540729+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/914906", 
"date_created": "2012-01-11T17:42:56.390169+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 914976, 
"date_in_progress": null, 
"bug_description": "Starting Percona XtraDB Cluster with following option cause crash\n\n\ncat /etc/my.cnf \n[mysqld]\ndatadir=/mnt/data/mysql\nuser=mysql\n\nlog_error=error.log\n\nbinlog_format=ROW\n\nwsrep_provider=/usr/lib64/libgalera_smm.so\n\nwsrep_cluster_address=gcomm://ec2-79-125-75-115.eu-west-1.compute.amazonaws.com\n\nwsrep_sst_receive_address=ec2-175-41-152-75.ap-southeast-1.compute.amazonaws.com\nwsrep_provider_options = \"ec2-175-41-152-75.ap-southeast-1.compute.amazonaws.com; ist.recv_addr=ec2-175-41-152-75.ap-southeast-1.compute.amazonaws.com\"\n\nwsrep_slave_threads=2\nwsrep_cluster_name=trimethylxanthine\nwsrep_sst_method=rsync\nwsrep_node_name=node2\n\ninnodb_locks_unsafe_for_binlog=1\ninnodb_autoinc_lock_mode=2\n\ninnodb_log_file_size=64M\n\n\n\nlog file:\n120111 14:31:20 mysqld_safe Starting mysqld daemon with databases from /mnt/data/mysql\n120111 14:31:20 [Note] Flashcache bypass: disabled\n120111 14:31:20 [Note] Flashcache setup error is : ioctl failed\n\n120111 14:31:20 [Note] WSREP: wsrep_load(): loading provider library '/usr/lib64/libgalera_smm.so'\n120111 14:31:20 [Note] WSREP: wsrep_load(): Galera 2.0beta(r103) by Codership Oy <email address hidden> loaded succesfully.\nterminate called after throwing an instance of 'std::length_error'\n  what():  basic_string::_S_create\n120111 14:31:21 - mysqld got signal 6 ;\nThis could be because you hit a bug. It is also possible that this binary\nor one of the libraries it was linked against is corrupt, improperly built,\nor misconfigured. This error can also be caused by malfunctioning hardware.\nWe will try our best to scrape up some info that will hopefully help diagnose\nthe problem, but since we have already crashed, something is definitely wrong\nand this may fail.\n\nkey_buffer_size=0\nread_buffer_size=131072\nmax_used_connections=0\nmax_threads=151\nthread_count=0\nconnection_count=0\nIt is possible that mysqld could use up to \nkey_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 330456 K\nbytes of memory\nHope that's ok; if not, decrease some variables in the equation.\n\nThread pointer: 0x0\nAttempting backtrace. You can use the following information to find out\nwhere mysqld died. If you see no messages after this, something went\nterribly wrong...\nstack_bottom = (nil) thread_stack 0x40000\n/usr/sbin/mysqld(my_print_stacktrace+0x39)[0x7b7c69]\n/usr/sbin/mysqld(handle_segfault+0x464)[0x514094]\n/lib64/libpthread.so.0(+0xf4a0)[0x7f9d97fa34a0]\n/lib64/libc.so.6(gsignal+0x35)[0x7f9d9715d885]\n/lib64/libc.so.6(abort+0x175)[0x7f9d9715f065]\n/usr/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x12d)[0x7f9d957f2a7d]\n/usr/lib64/libstdc++.so.6(+0xbcc06)[0x7f9d957f0c06]\n/usr/lib64/libstdc++.so.6(+0xbcc33)[0x7f9d957f0c33]\n/usr/lib64/libstdc++.so.6(+0xbcc46)[0x7f9d957f0c46]\n/usr/lib64/libstdc++.so.6(__cxa_call_unexpected+0x43)[0x7f9d957f02d3]\n/usr/lib64/libgalera_smm.so(_ZN2gu6Config5parseERSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERS5_+0x923)[0x7f9d95d18983]\n/usr/lib64/libgalera_smm.so(_ZN2gu6ConfigC2ERKSs+0x3e)[0x7f9d95d18b5e]\n/usr/lib64/libgalera_smm.so(_ZN6galera13ReplicatorSMMC1EPK15wsrep_init_args+0x5f)[0x7f9d95e31d4f]\n/usr/lib64/libgalera_smm.so(galera_init+0x34)[0x7f9d95e46594]\n/usr/sbin/mysqld(_Z10wsrep_initv+0x22e)[0x64ed8e]\n/usr/sbin/mysqld(_Z18wsrep_init_startupb+0x10)[0x64ef90]\n/usr/sbin/mysqld[0x51c517]\n/usr/sbin/mysqld(_Z11mysqld_mainiPPc+0xac1)[0x51fb51]\n/lib64/libc.so.6(__libc_start_main+0xfd)[0x7f9d97149cdd]\n/usr/sbin/mysqld[0x512929]\nThe manual page at http://dev.mysql.com/doc/mysql/en/crashing.html contains\ninformation that should help you find out what is causing the crash.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/914976/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Crash in EC2 node", 
"title": "Bug #914976 in Percona XtraDB Cluster: \"Crash in EC2 node\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/914976", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/914976", 
"milestone_link": null, 
"http_etag": "\"9b4d5a7f30ab5494a46210b3db0af0bcbce27bfc-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~percona-team", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/914976", 
"date_created": "2012-01-11T19:34:52.752388+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-01-20T06:14:37.916267+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-01-18T08:28:16.254179+00:00", 
"bug_id": 918060, 
"date_in_progress": "2012-01-18T08:28:16.254179+00:00", 
"bug_description": "Add Percona XtraDB Cluster Feature 1: High Availability to documentation", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/918060/related_tasks", 
"date_assigned": "2012-01-18T08:27:28.072712+00:00", 
"bug_information_type": "Public", 
"bug_title": "add HA feature to docs", 
"title": "Bug #918060 in Percona XtraDB Cluster: \"add HA feature to docs\"", 
"date_confirmed": "2012-01-18T08:28:16.254179+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/918060", 
"date_left_new": "2012-01-18T08:28:16.254179+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/918060", 
"milestone_link": null, 
"http_etag": "\"805111e866a8a10ca230e34f0ca61fe63c4a8b5c-1676e2f6e46a5cfd249281d11d93af620273de84\"", 
"owner_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-01-18T14:26:05.024874+00:00", 
"date_fix_released": "2012-01-20T06:14:37.916267+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/918060", 
"date_created": "2012-01-18T08:26:54.532502+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": null, 
"status": "In Progress", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-01-19T18:35:36.614849+00:00", 
"bug_id": 918218, 
"date_in_progress": "2012-01-19T18:35:36.614849+00:00", 
"bug_description": "I'm having a problem similar to this one: https://bugs.launchpad.net/codership-mysql/+bug/797396\n\nEnvironment: Percona XtraDB cluster, installed from Percona repo on RHEL 5, wsrep config taken from http://www.percona.com/doc/percona-xtradb-cluster/3nodesec2.html\n\nWhen I'm trying to join second node (node01) I see this error on second node:\n\n120118  9:07:32 [Note] WSREP: Prepared IST receiver, listening at: tcp://192.168.100.220:4568\n120118  9:07:32 [Note] WSREP: Node 1 (node01) requested state transfer from '*any*'. Selected 0 (node00)(SYNCED) as donor.\n120118  9:07:32 [Note] WSREP: Shifting PRIMARY -> JOINER (TO: 0)\n120118  9:07:32 [Note] WSREP: Requesting state transfer: success, donor: 0\n120118  9:07:33 [ERROR] WSREP: Failed to parse uuid:seqno pair: 'rsync process ended without creating '/var/lib/mysql//rsync_sst_complete''\n120118  9:07:33 [ERROR] WSREP: SST failed: 22 (Invalid argument)\n120118  9:07:33 [ERROR] Aborting\n\nAnd I see following error on first node (node00) when trying to join second one:\n\n\n120118  8:10:50 [ERROR] WSREP: Failed to read from: wsrep_sst_rsync 'donor' '192.168.100.220:4444/rsync_sst' '(null)' '/var/lib/mysql/' '/etc/my.cnf' 'd75c72ca-41c4-11e1-0800-20f251b58169' '0' '0' 2>sst.err\n\n120118  8:10:50 [ERROR] WSREP: Process completed with error: wsrep_sst_rsync 'donor' '192.168.100.220:4444/rsync_sst' '(null)' '/var/lib/mysql/' '/etc/my.cnf' 'd75c72ca-41c4-11e1-0800-20f251b58169' '0' '0' 2>sst.err: 12 (Cannot allocate memory)\n\nboth nodes have enough memory and basic configuration of wsrep. Please help to figure it out - is this a bug or misconfiguration issue or something.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/918218/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "rsync SST script returns confusing error code and little diagnostic on rsync protocol mismatch", 
"title": "Bug #918218 in Percona XtraDB Cluster: \"rsync SST script returns confusing error code and little diagnostic on rsync protocol mismatch\"", 
"date_confirmed": "2012-01-19T18:35:36.614849+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/918218", 
"date_left_new": "2012-01-19T18:35:36.614849+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/918218", 
"milestone_link": null, 
"http_etag": "\"6fb0cfff18d8734bb39d567a6ef115011aa464af-990b344f56c89e1a722d45dcd52e6c8dd1459a43\"", 
"owner_link": "https://api.launchpad.net/devel/~yuris", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/918218", 
"date_created": "2012-01-18T14:51:38.016795+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-01-30T20:00:45.528168+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": null, 
"bug_id": 918568, 
"date_in_progress": null, 
"bug_description": "document setting up cluster without state transfer", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/918568/related_tasks", 
"date_assigned": "2012-01-19T07:35:44.756681+00:00", 
"bug_information_type": "Public", 
"bug_title": "setting up cluster without state transfer", 
"title": "Bug #918568 in Percona XtraDB Cluster: \"setting up cluster without state transfer\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/918568", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/918568", 
"milestone_link": null, 
"http_etag": "\"1b396f371e85c14bff2b7a7c4da2f7b124488c3a-1676e2f6e46a5cfd249281d11d93af620273de84\"", 
"owner_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-01-20T07:34:55.371125+00:00", 
"date_fix_released": "2012-01-30T20:00:45.528168+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/918568", 
"date_created": "2012-01-19T07:35:44.424925+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_triaged": null, 
"bug_id": 919002, 
"date_in_progress": null, 
"bug_description": "In Node A, Node B, Node C setup\nI put tpcc-mysql load on the Node A.\n\nMeantime on the node B I execute:\n\nset global wsrep_OSU_method=RSU;\nalter table stock add (ii2 int);\n\nAfter some period of time the Node B crashed.\n\nLogs:\n\nNode B:\n120119 19:46:02 [Note] WSREP: Synchronized with group, ready for connections\n120119 19:46:29 [Note] WSREP: Node 1 (node2) desyncs itself from group\n120119 19:46:29 [Note] WSREP: Shifting SYNCED -> DONOR/DESYNCED (TO: 11077)\n120119 19:46:29 [Note] WSREP: MDL BF-BF conflict\nrequest: (3     seqno 11078     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:46:29 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 11079     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:46:29 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 11083     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:46:29 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 11087     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:46:29 [Note] WSREP: MDL BF-BF conflict\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:18 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13313     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:18 [Note] WSREP: MDL BF-BF conflict\nrequest: (3     seqno 13314     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:18 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13315     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\nrequest: (3     seqno 13320     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:18 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13321     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:18 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13327     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13333     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13335     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (3     seqno 13338     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13339     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (3     seqno 13342     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (3     seqno 13346     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP: MDL BF-BF conflict\nrequest: (2     seqno 13347     mode 1  Qstate  1 cmd 0 140     (null))\ngranted: (9     seqno 0         mode 2  Qstate  1 cmd 3 3       alter table stock add (ii2 int))\n120119 19:47:19 [Note] WSREP, BF applier interrupted in log_event.cc\n120119 19:47:19 [ERROR] Slave SQL: Error executing row event: 'Table 'tpcc1.stock' doesn't exist', Error_code: 1053\n120119 19:47:19 [Warning] WSREP: RBR event 8 Update_rows apply warning: 1053, 13347\n120119 19:47:19 [ERROR] WSREP: Failed to apply trx: source: 161996db-42fb-11e1-0800-335b92854e73 version: 1 local: 0 state: CERTIFYING flags: 129 conn_id: 37 trx_id: 42337 seqnos (l: 14083, g: 13347, s: 13345, d: 13346, ts: 1327020431308767982)\n120119 19:47:19 [ERROR] WSREP: Failed to apply app buffer: <8f>\u00b9^XO^S, seqno: 13347, status: WSREP_FATAL\n         at galera/src/replicator_smm.cpp:apply_wscoll():51\n         at galera/src/replicator_smm.cpp:apply_trx_ws():122\n120119 19:47:19 [ERROR] WSREP: Node consistency compromized, aborting...\n120119 19:47:19 [Note] WSREP: Closing send monitor...\n120119 19:47:19 [Note] WSREP: Closed send monitor.\n120119 19:47:19 [Note] WSREP: gcomm: terminating thread\n120119 19:47:19 [Note] WSREP: gcomm: joining thread\n120119 19:47:19 [Note] WSREP: gcomm: closing backend\n120119 19:47:19 [Note] WSREP: evs::proto(5819a7e8-42fb-11e1-0800-0a650aca7460, LEAVING, view_id(REG,161996db-42fb-11e1-0800-335b92854e73,3)) uuid 161996db-42fb-11e1-0800-335b92854e73 missing from install message, assuming partitioned\n120119 19:47:19 [Note] WSREP: evs::proto(5819a7e8-42fb-11e1-0800-0a650aca7460, LEAVING, view_id(REG,161996db-42fb-11e1-0800-335b92854e73,3)) uuid 9914c823-42fb-11e1-0800-5360821ace3c missing from install message, assuming partitioned\n120119 19:47:19 [Note] WSREP: New COMPONENT: primary = no, my_idx = 0, memb_num = 1\n120119 19:47:19 [Note] WSREP: GMCast::handle_stable_view: view(view_id(NON_PRIM,161996db-42fb-11e1-0800-335b92854e73,3) memb {\n        5819a7e8-42fb-11e1-0800-0a650aca7460,\n} joined {\n} left {\n} partitioned {\n        161996db-42fb-11e1-0800-335b92854e73,\n        9914c823-42fb-11e1-0800-5360821ace3c,\n})\n120119 19:47:19 [Note] WSREP: GMCast::handle_stable_view: view((empty))\n120119 19:47:19 [Note] WSREP: gcomm: closed\n120119 19:47:19 [Note] WSREP: Flow-control interval: [8, 16]\n120119 19:47:19 [Note] WSREP: Received NON-PRIMARY.\n120119 19:47:19 [Note] WSREP: Shifting DONOR/DESYNCED -> OPEN (TO: 13788)\n120119 19:47:19 [Note] WSREP: Received self-leave message.\n120119 19:47:19 [Note] WSREP: Flow-control interval: [0, 0]\n120119 19:47:19 [Note] WSREP: Received SELF-LEAVE. Closing connection.\n120119 19:47:19 [Note] WSREP: Shifting OPEN -> CLOSED (TO: 13788)\n120119 19:47:19 [Note] WSREP: RECV thread exiting 0: Success\n120119 19:47:19 [Note] WSREP: recv_thread() joined.\n120119 19:47:19 [Note] WSREP: Closing slave action queue.\n120119 19:47:19 [Note] WSREP: /usr/local/mysql/bin/mysqld: Terminated.\n\nNode A:\n120119 19:46:29 [Note] WSREP: Node 1 (node2) desyncs itself from group\n120119 19:47:19 [Note] WSREP: GMCast::handle_stable_view: view(view_id(PRIM,161996db-42fb-11e1-0800-335b92854e73,4) memb {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0161996db-42fb-11e1-0800-335b92854e73,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09914c823-42fb-11e1-0800-5360821ace3c,\n} joined {\n} left {\n} partitioned {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05819a7e8-42fb-11e1-0800-0a650aca7460,\n})\n120119 19:47:19 [Note] WSREP: forgetting 5819a7e8-42fb-11e1-0800-0a650aca7460 (tcp://10.171.137.235:4567)\n120119 19:47:19 [Note] WSREP: declaring 9914c823-42fb-11e1-0800-5360821ace3c stable\n120119 19:47:19 [Note] WSREP: (161996db-42fb-11e1-0800-335b92854e73, 'tcp://0.0.0.0:4567') turning message relay requesting off\n120119 19:47:19 [Note] WSREP: New COMPONENT: primary = yes, my_idx = 0, memb_num = 2\n120119 19:47:19 [Note] WSREP: STATE_EXCHANGE: sent state UUID: 4e9c3310-4300-11e1-0800-da117946fecf\n120119 19:47:19 [Note] WSREP: STATE EXCHANGE: sent state msg: 4e9c3310-4300-11e1-0800-da117946fecf\n120119 19:47:19 [Note] WSREP: STATE EXCHANGE: got state msg: 4e9c3310-4300-11e1-0800-da117946fecf from 0 (node1)\n120119 19:47:19 [Note] WSREP: STATE EXCHANGE: got state msg: 4e9c3310-4300-11e1-0800-da117946fecf from 1 (node3)\n120119 19:47:19 [Note] WSREP: Quorum results:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0version    = 2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0component  = PRIMARY,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0conf_id    = 3,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0members    = 2/2 (joined/total),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0act_id     = 13788,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0last_appl. = 13768,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0protocols  = 0/2/1 (gcs/repl/appl),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0group UUID = 161b1a80-42fb-11e1-0800-16f10a7153c5\n120119 19:47:19 [Note] WSREP: Flow-control interval: [12, 23]\n120119 19:47:19 [Note] WSREP: New cluster view: global state: 161b1a80-42fb-11e1-0800-16f10a7153c5:13788, view# 4: Primary, number of nodes: 2, my index: 0, protocol version 1\n120119 19:47:19 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120119 19:47:19 [Note] WSREP: Assign initial position for certification: 13788, protocol version: 1\n120119 19:47:19 [Note] WSREP: remote endpoint tcp://10.171.137.235:4567 changed identity 5819a7e8-42fb-11e1-0800-0a650aca7460 -> 4eb2e51c-4300-11e1-0800-c2085ad24748\n120119 19:47:20 [Note] WSREP: GMCast::handle_stable_view: view(view_id(PRIM,161996db-42fb-11e1-0800-335b92854e73,5) memb {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0161996db-42fb-11e1-0800-335b92854e73,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04eb2e51c-4300-11e1-0800-c2085ad24748,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09914c823-42fb-11e1-0800-5360821ace3c,\n} joined {\n} left {\n} partitioned {\n})\n120119 19:47:20 [Note] WSREP: declaring 4eb2e51c-4300-11e1-0800-c2085ad24748 stable\n120119 19:47:20 [Note] WSREP: declaring 9914c823-42fb-11e1-0800-5360821ace3c stable\n120119 19:47:20 [Note] WSREP: New COMPONENT: primary = yes, my_idx = 0, memb_num = 3\n120119 19:47:20 [Note] WSREP: STATE_EXCHANGE: sent state UUID: 4f5e7f8c-4300-11e1-0800-a6b72586305b\n120119 19:47:20 [Note] WSREP: STATE EXCHANGE: sent state msg: 4f5e7f8c-4300-11e1-0800-a6b72586305b\n120119 19:47:20 [Note] WSREP: STATE EXCHANGE: got state msg: 4f5e7f8c-4300-11e1-0800-a6b72586305b from 0 (node1)\n120119 19:47:20 [Note] WSREP: STATE EXCHANGE: got state msg: 4f5e7f8c-4300-11e1-0800-a6b72586305b from 2 (node3)\n120119 19:47:21 [Note] WSREP: STATE EXCHANGE: got state msg: 4f5e7f8c-4300-11e1-0800-a6b72586305b from 1 (node2)", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/919002/related_tasks", 
"date_assigned": "2012-01-20T00:54:58.108267+00:00", 
"bug_information_type": "Public", 
"bug_title": "Crash during ALTER TABLE using RSU", 
"title": "Bug #919002 in Percona XtraDB Cluster: \"Crash during ALTER TABLE using RSU\"", 
"date_confirmed": "2012-01-20T00:55:03.273087+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/919002", 
"date_left_new": "2012-01-20T00:55:03.273087+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/919002", 
"milestone_link": null, 
"http_etag": "\"088deb4e7e64f247515c500319352fba0ef4e09b-75cda6d3d5c0e29f7fc4a43f268830b445ae2a7b\"", 
"owner_link": "https://api.launchpad.net/devel/~vadim-tk", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/919002", 
"date_created": "2012-01-20T00:53:16.637231+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-01-30T20:02:36.378722+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-01-20T06:13:54.975789+00:00", 
"bug_id": 919065, 
"date_in_progress": "2012-01-20T06:13:54.975789+00:00", 
"bug_description": "add new blogpost about Multi-Master replication", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/919065/related_tasks", 
"date_assigned": "2012-01-20T06:13:58.690959+00:00", 
"bug_information_type": "Public", 
"bug_title": "Multi-Master replication", 
"title": "Bug #919065 in Percona XtraDB Cluster: \"Multi-Master replication\"", 
"date_confirmed": "2012-01-20T06:13:54.975789+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/919065", 
"date_left_new": "2012-01-20T06:13:54.975789+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/919065", 
"milestone_link": null, 
"http_etag": "\"25132a938a9de8759e48ac7daee2ae4a199e3d38-1676e2f6e46a5cfd249281d11d93af620273de84\"", 
"owner_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-01-20T14:40:38.793293+00:00", 
"date_fix_released": "2012-01-30T20:02:36.378722+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/919065", 
"date_created": "2012-01-20T06:13:46.751670+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-09-27T01:13:12.620879+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-09-27T01:13:12.620879+00:00", 
"bug_id": 923847, 
"date_in_progress": "2012-09-27T01:13:12.620879+00:00", 
"bug_description": "I have tested my application on a 2 node cluster.  Size of the test db is 100 GB.  Import was successful, and it the application run for  a few.  Then node1 reports :\n\nInnoDB: Error: semaphore wait has lasted > 600 seconds\nInnoDB: We intentionally crash the server, because it appears to be hung.\n120126 14:42:55  InnoDB: Assertion failure in thread 139823043221248 in file srv0srv.c line 2906", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/923847/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "semaphore wait has lasted > 600 seconds: intentionally crash the server", 
"title": "Bug #923847 in Percona XtraDB Cluster: \"semaphore wait has lasted > 600 seconds: intentionally crash the server\"", 
"date_confirmed": "2012-09-27T01:13:12.620879+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/923847", 
"date_left_new": "2012-09-27T01:13:12.620879+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/923847", 
"milestone_link": null, 
"http_etag": "\"7af62ad2765a470b4adfec95a937d0134389bf28-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~kzqdnsw4i2-peter-x8oxkp4um1", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-09-27T01:13:12.620879+00:00", 
"date_fix_released": "2012-09-27T01:13:12.620879+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/923847", 
"date_created": "2012-01-30T17:29:15.427380+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": "2012-02-07T22:49:27.876832+00:00", 
"status": "Invalid", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 927996, 
"date_in_progress": null, 
"bug_description": "When running kewpie test cases against codership-mysql 5.5 and percona-xtradb-cluster 5.5 with the latest galera-2.x library, I am seeing a number of failing test cases.\n\nThe cases are not the same for both versions of the server, but they are similar (indications of failed replication) output is below.\n\nTo repeat:\nfrom a fresh branch of lp:kewpie and the attached branch (merges in a config file + test cases).\n./kewpie.py --sys-config=codership-mysql-basedir/qp/qp-config.py --wsrep-provider-path=/path (default=/usr/lib/galera/libgalera_smm.so)\n\nThe qp directory includes a patch for the tests that will alter the tearDown behavior to leave the test schema intact if only one test runs / on the first failure...this had caused some problems previously.  \n\n20120206-184952  cluster_basic.replaceMultiRow_test         [ fail ]    12258\n20120206-184952  test_replace (replaceMultiRow_test.basicTest) ... FAIL\n20120206-184952  ERROR\n20120206-184952  \n20120206-184952  ======================================================================\n20120206-184952  ERROR: test_replace (replaceMultiRow_test.basicTest)\n20120206-184952  ----------------------------------------------------------------------\n20120206-184952  Traceback (most recent call last):\n20120206-184952  File \"/home/pcrews/bzr/work/pxc-beta/codership-mysql/kewpie/lib/util/mysqlBaseTestCase.py\", line 54, in tearDown\n20120206-184952  self.assertEqual(retcode,0,result)\n20120206-184952  AssertionError: (<type 'exceptions.Exception'>, OperationalError(1047, 'Unknown command'))\n20120206-184952  \n20120206-184952  ======================================================================\n20120206-184952  FAIL: test_replace (replaceMultiRow_test.basicTest)\n20120206-184952  ----------------------------------------------------------------------\n20120206-184952  Traceback (most recent call last):\n20120206-184952  File \"/home/pcrews/bzr/work/pxc-beta/codership-mysql/qp/cluster_basic/replaceMultiRow_test.py\", line 62, in test_replace\n20120206-184952  self.assertEqual(master_slave_diff, None, master_slave_diff)\n20120206-184952  AssertionError: {'s2': [\"t1: master_checksum= (('test.t1', 2836003186L),) | slave_checksum= Error 1047: Unknown command\"]}\n20120206-184952  \n20120206-184952  ----------------------------------------------------------------------", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/927996/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Many kewpie replication test cases are failing", 
"title": "Bug #927996 in Percona XtraDB Cluster: \"Many kewpie replication test cases are failing\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/927996", 
"date_left_new": "2012-02-07T22:49:27.876832+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/927996", 
"milestone_link": null, 
"http_etag": "\"c3944357e2c8ec9e9c49b6c02e0f6693cd22435f-f7133afc7c13191d8a487e081592a5120d81cf56\"", 
"owner_link": "https://api.launchpad.net/devel/~patrick-crews", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/927996", 
"date_created": "2012-02-07T01:59:19.082194+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": "2012-04-24T16:17:50.846955+00:00", 
"status": "Invalid", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 931561, 
"date_in_progress": null, 
"bug_description": "I was following the 3 node setup in singlebox in my lab instance (CentOS 5), just as described in \nhttp://www.percona.com/doc/percona-xtradb-cluster/singlebox.html\n\nThe SST method is rsync\n\nFirst node starts without any problem:\n\nExtract from show status like 'wsrep%':\n\n| wsrep_cluster_state_uuid   | da848620-53fa-11e1-0800-19d93dacb492 |\n| wsrep_cluster_status       | Primary                              |\n| wsrep_connected            | ON                                   |\n| wsrep_local_index          | 0                                    |\n| wsrep_provider_name        | Galera                               |\n| wsrep_provider_vendor      | Codership Oy <email address hidden>    |\n| wsrep_provider_version     | 2.0beta(rXXXX)                       |\n| wsrep_ready                | ON                                   |\n+----------------------------+--------------------------------------+\n\n\nOpen ports:\n\ntcp        0      0 0.0.0.0:4000                0.0.0.0:*                   LISTEN      29437/mysqld        \ntcp        0      0 0.0.0.0:4010                0.0.0.0:*                   LISTEN      29437/mysqld  \n\n\nWhen trying to start second node to join into the cluster, evertyhing seems fine (wsrep_sst_rsync, SST request, etc) but at the end of the log says:\n\n [Note] WSREP: SST received: da848620-53fa-11e1-0800-19d93dacb492:0\nterminate called after throwing an instance of 'gu::Exception'\n  what():  interrupted by ctrl: 4 (Interrupted system call)\n\t at galera/src/ist.cpp:recv_handshake_response():336\n\nAnd aborted.\n\nPrimary node tries to reconect unsuccesfuly:\n\n120213 10:04:07 [Note] WSREP: (cea2bdc2-5652-11e1-0800-002f2e624285, 'tcp://0.0.0.0:4010') reconnecting to d72a5b08-5652-11e1-0800-a686a66b4bf5 (tcp://10.240.110.31:5010), attempt 480\n120213 10:04:37 [Note] WSREP: (cea2bdc2-5652-11e1-0800-002f2e624285, 'tcp://0.0.0.0:4010') reconnecting to d72a5b08-5652-11e1-0800-a686a66b4bf5 (tcp://10.240.110.31:5010), attempt 510\n\nI'm using the binaries for RHEL5\n\n\nComplete log:\n\n\n[root@mysql-labo01 mysql]# bin/mysqld --defaults-file=/etc/my.5000.cnf\n120213  9:27:38 [Note] WSREP: wsrep_load(): loading provider library '/usr/local/mysql/lib/libgalera_smm.so'\n120213  9:27:38 [Note] WSREP: wsrep_load(): Galera 2.0beta(rXXXX) by Codership Oy <email address hidden> loaded succesfully.\n120213  9:27:38 [Note] WSREP: Reusing existing '/data/bench/d2//galera.cache'.\n120213  9:27:38 [Note] WSREP: Passing config to GCS: gcache.dir = /data/bench/d2/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /data/bench/d2//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gmcast.listen_addr = tcp://0.0.0.0:5010; replicator.commit_order = 3\n120213  9:27:38 [Note] WSREP: wsrep_sst_grab()\n120213  9:27:38 [Note] WSREP: Start replication\n120213  9:27:38 [Note] WSREP: Found saved state: 00000000-0000-0000-0000-000000000000:-1\n120213  9:27:38 [Note] WSREP: Assign initial position for certification: -1, protocol version: -1\n120213  9:27:38 [Note] WSREP: Setting initial position to 00000000-0000-0000-0000-000000000000:-1\n120213  9:27:38 [Note] WSREP: protonet asio version 0\n120213  9:27:38 [Note] WSREP: backend: asio\n120213  9:27:38 [Note] WSREP: GMCast version 0\n120213  9:27:38 [Note] WSREP: (e1902662-564e-11e1-0800-620c62a38645, 'tcp://0.0.0.0:5010') listening at tcp://0.0.0.0:5010\n120213  9:27:38 [Note] WSREP: (e1902662-564e-11e1-0800-620c62a38645, 'tcp://0.0.0.0:5010') multicast: , ttl: 1\n120213  9:27:38 [Note] WSREP: EVS version 0\n120213  9:27:38 [Note] WSREP: PC version 0\n120213  9:27:38 [Note] WSREP: gcomm: connecting to group 'trimethylxanthine', peer '10.240.110.31:4010'\n120213  9:27:39 [Note] WSREP: GMCast::handle_stable_view: view(view_id(PRIM,2594eb66-564c-11e1-0800-19e0d6ca0192,2) memb {\n\t2594eb66-564c-11e1-0800-19e0d6ca0192,\n\te1902662-564e-11e1-0800-620c62a38645,\n} joined {\n} left {\n} partitioned {\n})\n120213  9:27:39 [Note] WSREP: declaring 2594eb66-564c-11e1-0800-19e0d6ca0192 stable\n120213  9:27:39 [Note] WSREP: gcomm: connected\n120213  9:27:39 [Note] WSREP: Changing maximum packet size to 64500, resulting msg size: 32636\n120213  9:27:39 [Note] WSREP: Shifting CLOSED -> OPEN (TO: 0)\n120213  9:27:39 [Note] WSREP: Opened channel 'trimethylxanthine'\n120213  9:27:39 [Note] WSREP: Waiting for SST to complete.\n120213  9:27:39 [Note] WSREP: New COMPONENT: primary = yes, my_idx = 1, memb_num = 2\n120213  9:27:39 [Note] WSREP: STATE EXCHANGE: Waiting for state UUID.\n120213  9:27:39 [Note] WSREP: STATE EXCHANGE: sent state msg: e1dd4a1e-564e-11e1-0800-867286936c4d\n120213  9:27:39 [Note] WSREP: STATE EXCHANGE: got state msg: e1dd4a1e-564e-11e1-0800-867286936c4d from 0 (node4000)\n120213  9:27:39 [Note] WSREP: STATE EXCHANGE: got state msg: e1dd4a1e-564e-11e1-0800-867286936c4d from 1 (node5000)\n120213  9:27:39 [Note] WSREP: Quorum results:\n\tversion    = 2,\n\tcomponent  = PRIMARY,\n\tconf_id    = 1,\n\tmembers    = 1/2 (joined/total),\n\tact_id     = 0,\n\tlast_appl. = -1,\n\tprotocols  = 0/2/1 (gcs/repl/appl),\n\tgroup UUID = da848620-53fa-11e1-0800-19d93dacb492\n120213  9:27:39 [Note] WSREP: Flow-control interval: [12, 23]\n120213  9:27:39 [Note] WSREP: Shifting OPEN -> PRIMARY (TO: 0)\n120213  9:27:39 [Note] WSREP: New cluster view: global state: da848620-53fa-11e1-0800-19d93dacb492:0, view# 2: Primary, number of nodes: 2, my index: 1, protocol version 1\n120213  9:27:39 [Warning] WSREP: Gap in state sequence. Need state transfer.\n120213  9:27:41 [Note] WSREP: Running: 'wsrep_sst_rsync 'joiner' '10.240.110.31:5020' 'root:' '/data/bench/d2/' '/etc/my.5000.cnf' '26094' 2>sst.err'\n120213  9:27:42 [Note] WSREP: Prepared SST request: rsync|10.240.110.31:5020/rsync_sst\n120213  9:27:42 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120213  9:27:42 [Note] WSREP: Assign initial position for certification: 0, protocol version: 1\n120213  9:27:42 [Note] WSREP: State transfer required: \n\tGroup state: da848620-53fa-11e1-0800-19d93dacb492:0\n\tLocal state: 00000000-0000-0000-0000-000000000000:-1\n120213  9:27:42 [Note] WSREP: Prepared IST receiver, listening at: tcp://10.240.110.31:5011\n120213  9:27:42 [Note] WSREP: Node 1 (node5000) requested state transfer from '*any*'. Selected 0 (node4000)(SYNCED) as donor.\n120213  9:27:42 [Note] WSREP: Shifting PRIMARY -> JOINER (TO: 0)\n120213  9:27:42 [Note] WSREP: Requesting state transfer: success, donor: 0\n120213  9:27:42 [Note] WSREP: 0 (node4000): State transfer to 1 (node5000) complete.\n120213  9:27:42 [Note] WSREP: Member 0 (node4000) synced with group.\n120213  9:27:43 [Note] WSREP: SST complete, seqno: 0\n120213  9:27:43 [Note] Plugin 'FEDERATED' is disabled.\n120213  9:27:43 InnoDB: The InnoDB memory heap is disabled\n120213  9:27:43 InnoDB: Mutexes and rw_locks use GCC atomic builtins\n120213  9:27:43 InnoDB: Compressed tables use zlib 1.2.3\n120213  9:27:43 InnoDB: Using Linux native AIO\n120213  9:27:43 InnoDB: Initializing buffer pool, size = 128.0M\n120213  9:27:43 InnoDB: Completed initialization of buffer pool\n120213  9:27:43 InnoDB: highest supported file format is Barracuda.\n120213  9:27:43  InnoDB: Waiting for the background threads to start\n120213  9:27:44 Percona XtraDB (http://www.percona.com) 1.1.8-20.1 started; log sequence number 1597945\n120213  9:27:44 [Note] Event Scheduler: Loaded 0 events\n120213  9:27:44 [Note] WSREP: Signalling provider to continue.\n120213  9:27:44 [Note] WSREP: Received SST: da848620-53fa-11e1-0800-19d93dacb492:0\n120213  9:27:44 [Note] bin/mysqld: ready for connections.\nVersion: '5.5.17-22.1'  socket: '/tmp/mysql.5000.sock'  port: 5000  Percona XtraDB Cluster (GPL), Release 22.1, Revision 3683 wsrep_22.3.r3683\n120213  9:27:44 [Note] WSREP: SST received: da848620-53fa-11e1-0800-19d93dacb492:0\nterminate called after throwing an instance of 'gu::Exception'\n  what():  interrupted by ctrl: 4 (Interrupted system call)\n\t at galera/src/ist.cpp:recv_handshake_response():336\nAbortado", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/931561/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Can't join second node probably due to rsync related issues (handshake interrupted by system call)", 
"title": "Bug #931561 in Percona XtraDB Cluster: \"Can't join second node probably due to rsync related issues (handshake interrupted by system call)\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/931561", 
"date_left_new": "2012-04-24T16:17:50.846955+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/931561", 
"milestone_link": null, 
"http_etag": "\"68c0e99dafa40ad6532e5528e83f0756ed956ff0-f7133afc7c13191d8a487e081592a5120d81cf56\"", 
"owner_link": "https://api.launchpad.net/devel/~nethalo", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/931561", 
"date_created": "2012-02-13T17:50:03.552175+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"cluster", 
"rsync", 
"sst"
]
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 931713, 
"date_in_progress": null, 
"bug_description": "Hi,\n\nTrying to remove a single node from a 3-node cluster via:\n\nSET GLOBAL wsrep_cluster_address='gcomm://';  (have also tested, with the same result - SET GLOBAL wsrep_cluster_address='';)\n\nusing Server version: 5.5.17-22.1-log Percona XtraDB Cluster (GPL), Release 22.1, Revision 3683 wsrep_22.3.r3683\n\nCommand hangs the server - command never returns - and the server is no longer accessible - I have to use a kill -9 to get the instance usable again.\n\nFrom the error log from the node being removed:\n120213 22:15:09 [Note] WSREP: Stop replication\n120213 22:15:09 [Note] WSREP: Closing send monitor...\n120213 22:15:09 [Note] WSREP: Closed send monitor.\n120213 22:15:09 [Note] WSREP: gcomm: terminating thread\n120213 22:15:09 [Note] WSREP: gcomm: joining thread\n120213 22:15:09 [Note] WSREP: gcomm: closing backend\n120213 22:15:09 [Note] WSREP: evs::proto(e0c81b4c-567e-11e1-0800-85ed415644d4, LEAVING, view_id(REG,5d6a7cdc-565a-11e1-0800-7dba6589ad90,92)) uuid 5d6a7cdc-565a-11e1-0800-7dba6589ad90 missing from install message, assuming partitioned\n120213 22:15:09 [Note] WSREP: evs::proto(e0c81b4c-567e-11e1-0800-85ed415644d4, LEAVING, view_id(REG,5d6a7cdc-565a-11e1-0800-7dba6589ad90,92)) uuid 6cd1e65e-567f-11e1-0800-377d6c6676ab missing from install message, assuming partitioned\n120213 22:15:09 [Note] WSREP: GMCast::handle_stable_view: view(view_id(NON_PRIM,5d6a7cdc-565a-11e1-0800-7dba6589ad90,92) memb {\n        e0c81b4c-567e-11e1-0800-85ed415644d4,\n} joined {\n} left {\n} partitioned {\n        5d6a7cdc-565a-11e1-0800-7dba6589ad90,\n        6cd1e65e-567f-11e1-0800-377d6c6676ab,\n})\n120213 22:15:09 [Note] WSREP: New COMPONENT: primary = no, my_idx = 0, memb_num = 1\n120213 22:15:09 [Note] WSREP: GMCast::handle_stable_view: view((empty))\n120213 22:15:09 [Note] WSREP: gcomm: closed\n120213 22:15:09 [Note] WSREP: Flow-control interval: [230, 256]\n120213 22:15:09 [Note] WSREP: Received NON-PRIMARY.\n120213 22:15:09 [Note] WSREP: Shifting SYNCED -> OPEN (TO: 4046094)\n120213 22:15:09 [Note] WSREP: Received self-leave message.\n120213 22:15:09 [Note] WSREP: New cluster view: global state: 0ec148da-46c0-11e1-0800-e762c70be5e7:4046094, view# -1: non-Primary, number of nodes: 1, my index: 0, protocol version 1\n120213 22:15:09 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120213 22:15:09 [Note] WSREP: Flow-control interval: [0, 0]\n120213 22:15:09 [Note] WSREP: Received SELF-LEAVE. Closing connection.\n120213 22:15:09 [Note] WSREP: Shifting OPEN -> CLOSED (TO: 4046094)\n120213 22:15:09 [Note] WSREP: RECV thread exiting 0: Success\n120213 22:15:09 [Note] WSREP: recv_thread() joined.\n120213 22:15:09 [Note] WSREP: Closing slave action queue.\n120213 22:15:09 [Note] WSREP: New cluster view: global state: 0ec148da-46c0-11e1-0800-e762c70be5e7:4046094, view# -1: non-Primary, number of nodes: 0, my index: -1, protocol version 1\n120213 22:15:09 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:0)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:09 [Note] WSREP: applier thread exiting (code:5)\n120213 22:15:11 [Note] WSREP: rollbacker thread exiting\n\n\nContrast this with a valid gcomm address (which works as expected although it's not removing the node from the cluster):\n\nmysql> set global wsrep_cluster_address='gcomm://12.23.34.45:1260';\nQuery OK, 0 rows affected (3.51 sec)\n\n\nThanks\nPatrick", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/931713/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "removing node from a cluster via SET GLOBAL wsrep_cluster_address - hangs node being removed", 
"title": "Bug #931713 in Percona XtraDB Cluster: \"removing node from a cluster via SET GLOBAL wsrep_cluster_address - hangs node being removed\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/931713", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/931713", 
"milestone_link": null, 
"http_etag": "\"1d8c3f75a1cb204997af2cd2520d252addee72ae-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~patrickzoblisein", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/931713", 
"date_created": "2012-02-13T21:26:08.542979+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Incomplete", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-09-27T11:56:54.596004+00:00", 
"bug_id": 935049, 
"date_in_progress": "2012-09-27T11:56:54.596004+00:00", 
"bug_description": "Please kindly see more to go to the following URL on percona forum.\n\nhttp://forum.percona.com/index.php?t=msg&th=2099&start=0&S=b9e0a0ba443f99067f014db656163fe4", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/935049/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "startup occurs segmentation fault", 
"title": "Bug #935049 in Percona XtraDB Cluster: \"startup occurs segmentation fault\"", 
"date_confirmed": "2012-09-27T11:56:54.596004+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/935049", 
"date_left_new": "2012-09-27T11:56:54.596004+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/935049", 
"milestone_link": null, 
"http_etag": "\"4052f48bd197c47e43e352c3a73644028643f1ce-d658fc37755fb0720c19af892d4b2aa6fc16e37d\"", 
"owner_link": "https://api.launchpad.net/devel/~vj-zhanghai", 
"date_left_closed": null, 
"date_incomplete": "2012-09-27T11:56:54.596004+00:00", 
"date_fix_committed": "2012-09-27T11:56:54.596004+00:00", 
"date_fix_released": "2012-09-27T11:56:54.596004+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/935049", 
"date_created": "2012-02-18T12:47:12.752213+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-02-24T19:15:09.469854+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-02-24T18:51:47.665717+00:00", 
"bug_id": 940520, 
"date_in_progress": "2012-02-24T18:51:47.665717+00:00", 
"bug_description": "[In:Distribution-specific installation notes]\nhttp://www.percona.com/doc/percona-xtradb-cluster/installation/bin_distro_specific.html\nthis page should be removed", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/940520/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "[DOC] Suse install docs is irrelevant for XtraDB Cluster", 
"title": "Bug #940520 in Percona XtraDB Cluster: \"[DOC] Suse install docs is irrelevant for XtraDB Cluster\"", 
"date_confirmed": "2012-02-24T18:51:47.665717+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/940520", 
"date_left_new": "2012-02-24T18:51:47.665717+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/940520", 
"milestone_link": null, 
"http_etag": "\"8337b7a158c2457999bd3a4f7ed40b3dd0d76849-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~vadim-tk", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-02-24T18:51:47.665717+00:00", 
"date_fix_released": "2012-02-24T19:15:09.469854+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/940520", 
"date_created": "2012-02-24T18:27:50.872527+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-09-27T02:44:24.306506+00:00", 
"status": "Invalid", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 961914, 
"date_in_progress": null, 
"bug_description": "I followed the instructions at http://www.percona.com/doc/percona-xtradb-cluster/3nodesec2.html to set up Percona XtraDB Cluster on EC2. Within the same Amazon Region, it works fine. When I try one server in one region and one in another it doesn't.\n\nI have set wsrep_sst_auth to a user account with all privileges on both the initial machine and the node seeking to join it. After going through a state exchange I get Warning, failed to prepare for incremental state transfer: Local state UUID does not match group state UUID (Operation not permitted). Shortly after I get an error gcs_group_handle_join_msg() Will never receive state.\n\nI can Mysql between both machines using the credentials I created, as well as remotely accessing each machine individually. All ports are open and I can see movement in both terminals when I start the node up (they are communicating but failing to authorise?)\n\nI can supply more information as needed.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/961914/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Additional nodes will never receive state", 
"title": "Bug #961914 in Percona XtraDB Cluster: \"Additional nodes will never receive state\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/961914", 
"date_left_new": "2012-09-27T02:44:24.306506+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/961914", 
"milestone_link": null, 
"http_etag": "\"bce47369b4cef4950d69f01a423a94c4512285a9-f7133afc7c13191d8a487e081592a5120d81cf56\"", 
"owner_link": "https://api.launchpad.net/devel/~jameswithers89", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/961914", 
"date_created": "2012-03-22T07:20:40.690351+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": "2012-04-24T09:50:42.577496+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-04-11T14:07:35.873993+00:00", 
"bug_id": 977739, 
"date_in_progress": "2012-04-11T14:07:35.873993+00:00", 
"bug_description": "[In:Percona Toolkit Documentation]\n\n\nEmpty parameter wsrep_cluster_address on the /etc/my.4000.cnf on the  http://www.percona.com/doc/percona-xtradb-cluster/singlebox.html/ \n\nOther examples (/etc/my.5000.cnf and /etc/my.6000.cnf) contains wsrep_cluster_address=gcomm://10.11.12.205:4010\n\nIs it normal? \n\nThanks.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/977739/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "[DOC] Empty wsrep_cluster_address parameter in /etc/my.4000.cnf", 
"title": "Bug #977739 in Percona XtraDB Cluster: \"[DOC] Empty wsrep_cluster_address parameter in /etc/my.4000.cnf\"", 
"date_confirmed": "2012-04-11T14:07:35.873993+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/977739", 
"date_left_new": "2012-04-11T14:07:35.873993+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/977739", 
"milestone_link": null, 
"http_etag": "\"5995b26f9f7687ec17954451a847a908882fe507-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~eugene-irisov", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-04-11T14:07:35.873993+00:00", 
"date_fix_released": "2012-04-24T09:50:42.577496+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/977739", 
"date_created": "2012-04-10T05:54:14.404077+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-04-24T09:50:36.870022+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-04-11T14:07:50.094658+00:00", 
"bug_id": 978202, 
"date_in_progress": "2012-04-11T14:07:50.094658+00:00", 
"bug_description": "[In:Percona Toolkit Documentation]\n\nThere are conflicting configuration instructions on http://www.percona.com/doc/percona-xtradb-cluster/3nodesec2.html .  The my.cnf file makes mysqld run as the mysql user (which was created via RPM package) but by following installation instructions the /mnt/data directory is set up as root:root ownership.  I think the documentation should set the owner to at least mysql owner, like this:\n\nmysql_install_db --datadir=/mnt/data --user=mysql\n\nHope this helps,\n--\nMichael Coburn", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/978202/related_tasks", 
"date_assigned": "2012-04-11T09:38:04.489536+00:00", 
"bug_information_type": "Public", 
"bug_title": "[DOC] mysqld won't start in EC2 3 node setup with \"permission denied\" error message", 
"title": "Bug #978202 in Percona XtraDB Cluster: \"[DOC] mysqld won't start in EC2 3 node setup with \"permission denied\" error message\"", 
"date_confirmed": "2012-04-11T14:07:50.094658+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/978202", 
"date_left_new": "2012-04-11T14:07:50.094658+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/978202", 
"milestone_link": null, 
"http_etag": "\"b9c6a8d9d5241385fca64c76391535d8c6b3c965-1676e2f6e46a5cfd249281d11d93af620273de84\"", 
"owner_link": "https://api.launchpad.net/devel/~michaelcoburn", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-04-11T14:07:50.094658+00:00", 
"date_fix_released": "2012-04-24T09:50:36.870022+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/978202", 
"date_created": "2012-04-10T17:27:34.853389+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-04-24T09:50:30.415407+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-04-11T14:08:00.546562+00:00", 
"bug_id": 978224, 
"date_in_progress": "2012-04-11T14:08:00.546562+00:00", 
"bug_description": "[In:Percona Toolkit Documentation]\n\nHi,\n\nI don't know if this is an issue with just my ami (ami-e565ba8c) and t1.micro, but I needed to force the mysql client to take a username in order to log in to the mysqld instance with a user who had ALL PRIVILEGES, like this:\n\nmysql -uroot\n\nIf I left it as the documentation states, it logs in as the blank user from localhost which has only USAGE privilege to the database.\n--\nMichael Coburn", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/978224/related_tasks", 
"date_assigned": "2012-04-11T09:37:08.791325+00:00", 
"bug_information_type": "Public", 
"bug_title": "[DOC] unable to log in to mysqld using documentation", 
"title": "Bug #978224 in Percona XtraDB Cluster: \"[DOC] unable to log in to mysqld using documentation\"", 
"date_confirmed": "2012-04-11T14:08:00.546562+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/978224", 
"date_left_new": "2012-04-11T14:08:00.546562+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/978224", 
"milestone_link": null, 
"http_etag": "\"1cdb806b401d0eb3421029ce9c921d8118e8afe7-1676e2f6e46a5cfd249281d11d93af620273de84\"", 
"owner_link": "https://api.launchpad.net/devel/~michaelcoburn", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-04-11T14:08:00.546562+00:00", 
"date_fix_released": "2012-04-24T09:50:30.415407+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/978224", 
"date_created": "2012-04-10T17:59:22.574387+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": null, 
"bug_id": 987719, 
"date_in_progress": null, 
"bug_description": "I'm trying to set up XtraDB cluster on 3 machines and having trouble using the IST.\n\nAll machines have the following version installed:\n\nmysqld  Ver 5.5.20 for Linux on x86_64 (Percona XtraDB Cluster (GPL), wsrep_23.4.r3748)\n\nI kind of succeeded and got everything working fine (using public ip addresses, iptables off, selinux off), however I'm having problem to set up IST properly.\n\n------------------------------------------------------------------------------------------------------------------------------------------------\nnode-au-1\n------------------------------------------------------------------------------------------------------------------------------------------------\n\nI have the following line in  /etc/my.cnf on node 1 (ninefold.com's CentOS 6 instance):\n\nwsrep_provider_options = \"gmcast.listen_addr=tcp://0.0.0.0; ist.recv_addr=202.2.95.88;\"\n\nWhen I try to start the service using 'service mysql start' I get the following:\n\n120424 16:57:12 mysqld_safe Starting mysqld daemon with databases from /data\n120424 16:57:12 [Note] Flashcache bypass: disabled\n120424 16:57:12 [Note] Flashcache setup error is : ioctl failed\n\n120424 16:57:12 [Note] WSREP: wsrep_load(): loading provider library '/usr/lib64/libgalera_smm.so'\n120424 16:57:12 [Note] WSREP: wsrep_load(): Galera 2.1dev(r109) by Codership Oy <email address hidden> loaded succesfully.\n120424 16:57:12 [Note] WSREP: Reusing existing '/data//galera.cache'.\n120424 16:57:12 [Note] WSREP: Passing config to GCS: gcache.dir = /data/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /data//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gmcast.listen_addr = tcp://0.0.0.0; ist.recv_addr = 202.2.95.88; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n120424 16:57:12 [Warning] WSREP: Unknown parameter 'ist.recv_addr'\n...\n\nSo it looks like it ignores the ist.recv_addr setting. \n\nLet's check using mysql:\n\nmysql> SHOW VARIABLES LIKE 'wsrep_provider_options';\n+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Variable_name          | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n+------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| wsrep_provider_options | base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /data/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /data//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 10.8.224.202; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = false; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3 |\n+------------------------+-----------------------------------------\n\nOk, it did ignore it and seems to use 10.8.224.202 as a fallback (this is machine's private IP).\n\nIs this a bug?", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/987719/related_tasks", 
"date_assigned": "2012-05-01T02:19:09.876770+00:00", 
"bug_information_type": "Public", 
"bug_title": "[Warning] WSREP: Unknown parameter 'ist.recv_addr'", 
"title": "Bug #987719 in Percona XtraDB Cluster: \"[Warning] WSREP: Unknown parameter 'ist.recv_addr'\"", 
"date_confirmed": "2012-05-01T02:19:15.585011+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/987719", 
"date_left_new": "2012-05-01T02:19:15.585011+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/987719", 
"milestone_link": null, 
"http_etag": "\"4e17593e5852ab9c0b7db36871a4f72f1f1f550a-d08cb239a59f4ff7458c8232792271429dad4397\"", 
"owner_link": "https://api.launchpad.net/devel/~y-ludek", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/987719", 
"date_created": "2012-04-24T09:23:29.545549+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": "2012-06-08T17:57:27.340359+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-06-08T17:57:27.340359+00:00", 
"bug_id": 992711, 
"date_in_progress": "2012-06-08T17:57:27.340359+00:00", 
"bug_description": "https://bugs.launchpad.net/codership-mysql/+bug/959512\n\n120501 19:42:29 [ERROR] Slave SQL: Could not execute Write_rows event on table MainShard.Friends; Duplicate entry '1691359952-531712940' for key 'PRIMARY', Error_code: 1062; handler error HA_ERR_FOUND_DUPP_KEY; the event's master log FIRST, end_log_pos 1082, Error_code: 1062\n\nAfther this all cluster members are restarting, and getting back in sync, but for a short period of time (10-30 minutes)", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/992711/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Replication breaks, galera bug", 
"title": "Bug #992711 in Percona XtraDB Cluster: \"Replication breaks, galera bug\"", 
"date_confirmed": "2012-06-08T17:57:27.340359+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/992711", 
"date_left_new": "2012-06-08T17:57:27.340359+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/992711", 
"milestone_link": null, 
"http_etag": "\"ef35715a4388d52b93c7b0b0434701c3952a28da-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~andrianjardan", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-06-08T17:57:27.340359+00:00", 
"date_fix_released": "2012-06-08T17:57:27.340359+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/992711", 
"date_created": "2012-05-01T16:45:24.709519+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 992830, 
"date_in_progress": null, 
"bug_description": "I have a 3 node test cluster comprising nodes A, B, and C.\n\nI brought up Node A first, followed by B and C, and created a couple databases, which are synced across the cluster.\n\n$ dsh -e -g xtradb-cluster.ops \"/usr/local/mysql/bin/mysql -e 'show databases'\" | grep -v Database | grep -v schema | grep -v mysql\nxtradb-cluster101.ops: blah\nxtradb-cluster101.ops: tester\nxtradb-cluster102.ops: blah\nxtradb-cluster102.ops: tester\nxtradb-cluster103.ops: blah\nxtradb-cluster103.ops: tester\n\nI then cleanly shutdown node A and added a new database on node B, which synchronized to nodes B and C.\n\n$ dsh -e -g xtradb-cluster.ops \"/usr/local/mysql/bin/mysql -e 'show databases'\" | grep -v Database | grep -v schema | grep -v mysql\nxtradb-cluster102.ops: blah\nxtradb-cluster102.ops: tester\nxtradb-cluster102.ops: this_should_show_up_on_all_3_nodes\nxtradb-cluster103.ops: blah\nxtradb-cluster103.ops: tester\nxtradb-cluster103.ops: this_should_show_up_on_all_3_nodes\n\nI then brought node A back online.  I would expect the new database to exist on node A, but it does not.\n\n$ dsh -e -g xtradb-cluster.ops \"/usr/local/mysql/bin/mysql -e 'show databases'\" | grep -v Database | grep -v schema | grep -v mysql\nxtradb-cluster101.ops: blah\nxtradb-cluster101.ops: tester\nxtradb-cluster102.ops: blah\nxtradb-cluster102.ops: tester\nxtradb-cluster102.ops: this_should_show_up_on_all_3_nodes\nxtradb-cluster103.ops: blah\nxtradb-cluster103.ops: tester\nxtradb-cluster103.ops: this_should_show_up_on_all_3_nodes\n\nThe cluster is still in sync, somewhat, as I'm able to issue \"drop database this_should_show_up_on_all_3_nodes\" on node A, which then deletes the database on nodes B and C.\n\nHere are the relevant lines in my.cnf on all 3 nodes.  The only difference is on node A where gcomm:// is empty.\n\nwsrep_provider=/usr/local/mysql/lib/libgalera_smm.so\nwsrep_cluster_address=gcomm://10.202.1.37\nwsrep_slave_threads=8\nwsrep_sst_method=rsync\nwsrep_cluster_name=SuperAwesomeFusionTestCluster\nbinlog_format=ROW\ndefault_storage_engine=InnoDB\ninnodb_autoinc_lock_mode=2\ninnodb_locks_unsafe_for_binlog=1", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/992830/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "node won't synchronize new data from cluster following an outage on that node", 
"title": "Bug #992830 in Percona XtraDB Cluster: \"node won't synchronize new data from cluster following an outage on that node\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/992830", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/992830", 
"milestone_link": null, 
"http_etag": "\"1a925e3f156b84e3f71b06af4f82550074341447-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~mrfino", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/992830", 
"date_created": "2012-05-01T20:00:32.679860+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": null, 
"bug_id": 999492, 
"date_in_progress": null, 
"bug_description": "From what I can tell libtinfo.so has long been deprecated in favor of libncurses.so.  The binary builds of xtra server link against libncurses.so.5 why not xtradb cluster?\n\nThanks!", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/999492/related_tasks", 
"date_assigned": "2012-05-16T14:46:30.422117+00:00", 
"bug_information_type": "Public", 
"bug_title": "binary build mysql linked against libtinfo.so.5 instead of libncurses.so.5", 
"title": "Bug #999492 in Percona XtraDB Cluster: \"binary build mysql linked against libtinfo.so.5 instead of libncurses.so.5\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/999492", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/999492", 
"milestone_link": null, 
"http_etag": "\"3c0f5eb8fe297e9d413e098bead6232075128678-5821ea45e073cfa50e0e503f11dbee96b24f1ed1\"", 
"owner_link": "https://api.launchpad.net/devel/~travisghansen", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/999492", 
"date_created": "2012-05-15T06:48:21.278525+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1007554, 
"date_in_progress": null, 
"bug_description": "when using bonded network interfaces:\nWSREP: Failed to read output of: '/sbin/ifconfig | grep -m1 -1 -E '^[a-z]?eth[0-9]' | tail -n 1 | awk '{ print $2 }' | awk -F : '{ print $2 }''\n[Warning] WSREP: Failed to autoguess base node address\n[Note] WSREP: Service disconnected.\n[Note] WSREP: Some threads may fail to exit.\n\nThis might work better and should be interface type agnostic:\nping -i 1 -c 1 $(hostname) | grep -m1 -1 -E '\\([0-9.]+{4}\\)' | awk '{ print $3 }' | sed 's/[()]//g' | grep -v '127.0.0.1'\n\nThere should also probably be an option to set the preferred IP via an environment variable before installing the RPM.  It may be the case that multiple interfaces exist, and the administrator prefers to use a particular interface (not just the first one from the list, or the one associated with the hostname).  For unattended installations, setting the interface automatically may not work.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1007554/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "XtraDB cluster does not properly detect bonded network interfaces", 
"title": "Bug #1007554 in Percona XtraDB Cluster: \"XtraDB cluster does not properly detect bonded network interfaces\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1007554", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1007554", 
"milestone_link": null, 
"http_etag": "\"cf3bdc2e9e8dfe7175add1d09073c7d90f05d46c-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~greenlion", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1007554", 
"date_created": "2012-06-01T18:30:38.799218+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1008278, 
"date_in_progress": null, 
"bug_description": "How to repeat:\nSetup Master-Master replication on Percona Server 5.5, enable userstats on both instances.  Try to create database on both instances and the server will crash\n\nnode1:\nmysql> show global variables like 'userstat';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| userstat      | OFF   |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> set global userstat=1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> create database db1;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> create database db2;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> create database db6;\nERROR 2006 (HY000): MySQL server has gone away\nNo connection. Trying to reconnect...\nConnection id:    3\nCurrent database: *** NONE ***\n\nQuery OK, 1 row affected (0.00 sec)\n\nnode2:\nmysql> show global variables like 'userstat';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| userstat      | OFF   |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> create database db3;                   \nQuery OK, 1 row affected (0.00 sec)\n\nmysql> create database db4;\nQuery OK, 1 row affected (0.01 sec)\n\nmysql> set global userstat=1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> create database db5;\nQuery OK, 1 row affected (0.00 sec)\n\nStack trace:\nThread pointer: 0x7fb9b0000990\nAttempting backtrace. You can use the following information to find out\nwhere mysqld died. If you see no messages after this, something went\nterribly wrong...\nstack_bottom = 7fb9c63e7e78 thread_stack 0x40000\n/home/user/mysql_binaries/5.5.24/bin/mysqld(my_print_stacktrace+0x35)[0x7d6005]\n/home/user/mysql_binaries/5.5.24/bin/mysqld(handle_fatal_signal+0x3e1)[0x691651]\n/lib64/libpthread.so.0(+0xf4a0)[0x7fb9df2804a0]\n/home/user/mysql_binaries/5.5.24/bin/mysqld[0x61c266]\n/home/user/mysql_binaries/5.5.24/bin/mysqld(_Z24update_global_user_statsP3THDbl+0x516)[0x61cb76]\n/home/user/mysql_binaries/5.5.24/bin/mysqld(_Z11mysql_parseP3THDPcjP12Parser_state+0xae)[0x5809fe]\n/home/user/mysql_binaries/5.5.24/bin/mysqld(_ZN15Query_log_event14do_apply_eventEPK14Relay_log_infoPKcj+0x100b)[0x7554db]\n/home/user/mysql_binaries/5.5.24/bin/mysqld(_Z26apply_event_and_update_posP9Log_eventP3THDP14Relay_log_info+0x152)[0x518192]\n/home/user/mysql_binaries/5.5.24/bin/mysqld[0x5216b6]\n/home/user/mysql_binaries/5.5.24/bin/mysqld(handle_slave_sql+0xc19)[0x5229c9]\n/lib64/libpthread.so.0(+0x77f1)[0x7fb9df2787f1]\n/lib64/libc.so.6(clone+0x6d)[0x7fb9de4feccd]\n\nWorkaround:\nDisable userstat on one of the instances (SET GLOBAL userstat=0;)", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1008278/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Percona Server 5.5.24 crashes on userstats=1 with any replication configured", 
"title": "Bug #1008278 in Percona XtraDB Cluster: \"Percona Server 5.5.24 crashes on userstats=1 with any replication configured\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1008278", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1008278", 
"milestone_link": null, 
"http_etag": "\"d49fef846168fdeab3e4186582753e0bc0ac87f0-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~korolev-n", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1008278", 
"date_created": "2012-06-14T14:23:00.046499+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"i24006"
]
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1010026, 
"date_in_progress": null, 
"bug_description": "Hi, I'm running the latest:\n\npercona-xtradb-cluster-server-5.5 5.5.23-23.5-333.lucid\n\nI've tried repeatedly to replicate to a percona cluster consisting of two nodes. A duplicate primary key insert breaks replication. This cluster is at the end of of a chain of replicated servers, so it's definitely an issue with galera/percona cluster. Not sure how I can help reproduce this issue :(", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1010026/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "replication to a percona cluster breaks", 
"title": "Bug #1010026 in Percona XtraDB Cluster: \"replication to a percona cluster breaks\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1010026", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1010026", 
"milestone_link": null, 
"http_etag": "\"83264f16d60e27a0ad8a2b30ca3f90a2ba113b0d-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~geoff-flarity", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1010026", 
"date_created": "2012-06-07T14:04:59.441602+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1012138, 
"date_in_progress": null, 
"bug_description": "mysql> set global wsrep_provider_options=\"gcs.fc_master_slave=YES\";\nERROR 1210 (HY000): Incorrect arguments to SET\nmysql> show variables like 'wsrep_provider_options';\n+------------------------+-------------------------+\n| Variable_name          | Value                   |\n+------------------------+-------------------------+\n| wsrep_provider_options | gcs.fc_master_slave=YES |\n+------------------------+-------------------------+\n1 row in set (0.00 sec)\n\n\nTwo issues here:  \n- The 'incorrect arguments' error\n- When I set an invalid parameter in wsrep_provider_options, SHOW VARIABLES incorrectly shows me the bad setting, and not all the settings within wsrep_provider_options", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1012138/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "set gcs.fc_master_slave=YES gives error", 
"title": "Bug #1012138 in Percona XtraDB Cluster: \"set gcs.fc_master_slave=YES gives error\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1012138", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1012138", 
"milestone_link": null, 
"http_etag": "\"f53f1f161fad7642c69af38c7b673c80f1fa8258-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~jay-janssen", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1012138", 
"date_created": "2012-06-12T13:57:02.260446+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1012392, 
"date_in_progress": null, 
"bug_description": "Acording to the limitaions on this page http://www.percona.com/doc/percona-xtradb-cluster/limitation.html\nAny direct modifications on the mysql.user table would not be replicated but DDL statemens should be replicated. \n\nI tried this:\nset password for user = PASSWORD('somepassword')\n\nand it was not replicated to any node.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1012392/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "\"set password for user\" not replicated", 
"title": "Bug #1012392 in Percona XtraDB Cluster: \"\"set password for user\" not replicated\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1012392", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1012392", 
"milestone_link": null, 
"http_etag": "\"42b89f45d9510eb759b6600c397963e978d5f1c6-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~mihai-patchlog", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1012392", 
"date_created": "2012-06-12T21:44:04.517662+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1015083, 
"date_in_progress": null, 
"bug_description": "Hi,\n\nDuring my tests with the xtrabackup method for SST, I discovered one very particular case where the donor will not be able to complete successfully the transfer to the joining node because of the fact that the mysql configuration file parameter is not used only in donor mode within the bin/wsrep_sst_xtrabackup shell script.\n\nAlmost all informations are available on this thread :\nhttps://groups.google.com/forum/?fromgroups#!topic/percona-discussion/z4NDidmQNAw\n\nBasically here is a summary : on my 3 nodes cluster setup, my MySQL configuration files are named and located at /etc/my-galera.cnf which is non standard name. My nodes are not using a standard location also for the mysql socket. All of this seems to be a problem for the bin/wsrep_sst_xtrabackup shell script because it does not mention the mysql configuration location when $ROLE is set to donor (whereas it is done in joiner mode)\n\nHere is a sample of script around line 96 :\n\n        set +e\n        ${INNOBACKUPEX_BIN} --galera-info --tmpdir=${TMPDIR} --stream=tar ${TMPDIR} ${INNOBACKUPEX_ARGS} 2> ${DATA}/innobackup.backup.log | ${NC_BIN} ${REMOTEIP} ${NC_PORT}\n        RC=( \"${PIPESTATUS[@]}\" )\n        set -e\n\nThe parameter \"--defaults-file=${CONF}\" should be added to the INNOBACKUPEX_BIN line to avoid this situation so every special configuration related to the node will be passed to innobackupex binary.\n\nAt your disposal for any more informations needed.\nThanks.\n\nRegards,\n\nLaurent", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1015083/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "SST with xtrabackup method should provide mysql conf in donor mode", 
"title": "Bug #1015083 in Percona XtraDB Cluster: \"SST with xtrabackup method should provide mysql conf in donor mode\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1015083", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1015083", 
"milestone_link": null, 
"http_etag": "\"4a8fc508c47d702954a242aded94ac23dd0797c0-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~lolomin", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1015083", 
"date_created": "2012-06-19T11:17:10.524714+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-09-27T07:42:10.458006+00:00", 
"status": "Invalid", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1017046, 
"date_in_progress": null, 
"bug_description": "/usr/local/mysql/bin/mysqld: /lib64/libc.so.6: version `GLIBC_2.7' not found (required by /usr/local/mysql/bin/mysqld)\n\nRunning CentOS 5.7 on kernel-2.6.18-274.el5\n\nCan we get a backwards compatibility binary version as as to avoid having to rebuild glibc", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1017046/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Latest binary version built against GLIBC 2.7", 
"title": "Bug #1017046 in Percona XtraDB Cluster: \"Latest binary version built against GLIBC 2.7\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1017046", 
"date_left_new": "2012-09-27T07:42:10.458006+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1017046", 
"milestone_link": null, 
"http_etag": "\"60d257f9d8142dec775816b7d46dc4064598a546-f7133afc7c13191d8a487e081592a5120d81cf56\"", 
"owner_link": "https://api.launchpad.net/devel/~mrfino", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1017046", 
"date_created": "2012-06-24T04:21:13.301855+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1017526, 
"date_in_progress": null, 
"bug_description": "Documentation in /usr/bin/clustercheck states\n# \n# mysql> GRANT PROCESS on mysql.* TO 'clustercheckuser'@'localhost' \\ \n#     -> IDENTIFIED BY 'clustercheckpassword!' WITH GRANT OPTION; \n#\n\nThis will not work (Incorrect usage of DB grant and global privileges).\n\nA correct syntact would be something like \n\nGRANT PROCESS ON *.* TO 'clustercheckuser'@'localhost' IDENTIFIED BY 'clustercheckpassword!';", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1017526/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Grant documentation error in clustercheck", 
"title": "Bug #1017526 in Percona XtraDB Cluster: \"Grant documentation error in clustercheck\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1017526", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1017526", 
"milestone_link": null, 
"http_etag": "\"713bb3a95ba543e250837cee8fb154c67f096bba-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~olafz", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1017526", 
"date_created": "2012-06-25T14:25:15.095290+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-09-27T11:10:05.946603+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-09-27T11:10:05.946603+00:00", 
"bug_id": 1017528, 
"date_in_progress": "2012-09-27T11:10:05.946603+00:00", 
"bug_description": "On Debian linux, the clustercheck command returns to many newlines.\n\nThe lines in the scripts look like\n\n    /bin/echo -e \"HTTP/1.1 200 OK\\r\\n\" \n    /bin/echo -e \"Content-Type: Content-Type: text/plain\\r\\n\" \n    /bin/echo -e \"\\r\\n\" \n    /bin/echo -e \"Node is running.\\r\\n\" \n\nwhich returns (with all the newlines)\n\nHTTP/1.1 200 OK\n\nContent-Type: Content-Type: text/plain\n\n\n\nNode is running.\n\n\nReplacing \"echo -e\" by \"echo -en\" does not output the trailing newline and produces the right output:\n\nHTTP/1.1 200 OK\nContent-Type: Content-Type: text/plain\n\nNode is running.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1017528/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "clustercheck", 
"title": "Bug #1017528 in Percona XtraDB Cluster: \"clustercheck\"", 
"date_confirmed": "2012-09-27T11:10:05.946603+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1017528", 
"date_left_new": "2012-09-27T11:10:05.946603+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1017528", 
"milestone_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+milestone/percona-xtradb-cluster-5.5.27", 
"http_etag": "\"4d1e82f7aa8ed9245a2329f05be28cbae2996e5b-92d11bac609798ed68d286597172671c96e4bf6d\"", 
"owner_link": "https://api.launchpad.net/devel/~olafz", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-09-27T11:10:05.946603+00:00", 
"date_fix_released": "2012-09-27T11:10:05.946603+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1017528", 
"date_created": "2012-06-25T14:30:53.040225+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1018086, 
"date_in_progress": null, 
"bug_description": "in the configuration (my.cnf) if wsrep_urls is used in mysqld_safe section, I think it would be very nice that if the port is not specified, the default port (4567) is used.\n\nIndeed when you have something like :\n\n[mysqld_safe]\nwsrep_urls=gcomm://10.10.11.170,gcomm://10.10.57.235,gcomm://\n\nmysqld_safe in function wsrep_pick_url will bypass all the entries with IP's and not ports without any warnings and start a new cluster.\n\nThe expected configuration is :\n\nwsrep_urls=gcomm://10.10.11.170:4567,gcomm://10.10.57.235:4567,gcomm://\n\nbut I think it's very easy to add the default port and will avoid many errors.\n\nCheers,", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1018086/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "wsrep_urls should use default ports", 
"title": "Bug #1018086 in Percona XtraDB Cluster: \"wsrep_urls should use default ports\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1018086", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1018086", 
"milestone_link": null, 
"http_etag": "\"df7e55e8350b475b8ada78d3a4392b2d9351bcb3-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~lefred", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1018086", 
"date_created": "2012-06-26T19:44:14.377196+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-06-27T11:17:24.359907+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~hrvojem", 
"date_triaged": "2012-06-27T08:54:03.063481+00:00", 
"bug_id": 1018089, 
"date_in_progress": "2012-06-27T08:54:03.063481+00:00", 
"bug_description": "on http://www.percona.com/doc/percona-xtradb-cluster/resources.html, it states that \"If you use rsync or xtrabackup SST, wsrep_sst_auth is not necessary unless your SST script makes use of it\".\n\nBut if you use xtrabackup as SST method, it will use  /usr/bin/wsrep_sst_xtrabackup provided in Percona-XtraDB-Cluster-server package. And this script also needs user password if you have a password for root@localhost.\n\nSo I don't think it depends only on the SST script used but also about the user grants on the system.\n\nIn the code of wsrep_sst_xtrabackup we can see that innobackupex uses it:\n\n if [ ${#AUTH[*]} -eq 2 ]; then\n           INNOBACKUPEX_ARGS=\"${INNOBACKUPEX_ARGS} --password=${AUTH[1]}\"\n fi\n\n\nCheers", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1018089/related_tasks", 
"date_assigned": "2012-06-27T08:53:55.007713+00:00", 
"bug_information_type": "Public", 
"bug_title": "documentation related to wsrep_sst_auth can be confusing", 
"title": "Bug #1018089 in Percona XtraDB Cluster: \"documentation related to wsrep_sst_auth can be confusing\"", 
"date_confirmed": "2012-06-27T08:54:03.063481+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1018089", 
"date_left_new": "2012-06-27T08:54:03.063481+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1018089", 
"milestone_link": null, 
"http_etag": "\"38b569278e7a87bc45e3145cd461850b2dee5f08-1676e2f6e46a5cfd249281d11d93af620273de84\"", 
"owner_link": "https://api.launchpad.net/devel/~lefred", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-06-27T10:11:14.431809+00:00", 
"date_fix_released": "2012-06-27T11:17:24.359907+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1018089", 
"date_created": "2012-06-26T19:51:38.466676+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": [
"docs"
]
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1019288, 
"date_in_progress": null, 
"bug_description": "The wsrep_xtrabackup_sst script copies everything to datadir, and not to it's configured location.\n\nmysql> select @@innodb_log_group_home_dir;\n+-----------------------------+\n| @@innodb_log_group_home_dir |\n+-----------------------------+\n| /innodb_log                 |\n+-----------------------------+\n1 row in set (0.00 sec)\n\nmysql> select @@innodb_data_home_dir;\n+------------------------+\n| @@innodb_data_home_dir |\n+------------------------+\n| /innodb_data           |\n+------------------------+\n1 row in set (0.00 sec)\n\nAt sst time, the sst finishes without errors.\n\nAt startup time, new, empty data files and transaction logs are created.\n\n120629  9:38:15 [Note] WSREP: Shifting PRIMARY -> JOINER (TO: 42449)\n120629  9:38:15 [Note] WSREP: Requesting state transfer: success, donor: 0\n120629  9:38:53 [Note] WSREP: 0 (ip-10-112-39-98): State transfer to 1 (ip-10-244-33-92) complete.\n120629  9:38:53 [Note] WSREP: Member 0 (ip-10-112-39-98) synced with group.\n\nAfter that, since innodb_data_home_dir and innodb_log_group_home_dir is empty, innodb will create empty log and data files.\n\nInnoDB: The first specified data file /innodb_data/ibdata1 did not exist:\nInnoDB: a new database to be created!\n120629  9:39:09  InnoDB: Setting file /innodb_data/ibdata1 size to 10 MB\nInnoDB: Database physically writes the file full: wait...\n120629  9:39:09  InnoDB: Log file /innodb_log/ib_logfile0 did not exist: new to be created\nInnoDB: Setting log file /innodb_log/ib_logfile0 size to 64 MB\nInnoDB: Database physically writes the file full: wait...\n120629  9:39:12  InnoDB: Log file /innodb_log/ib_logfile1 did not exist: new to be created\nInnoDB: Setting log file /innodb_log/ib_logfile1 size to 64 MB\nInnoDB: Database physically writes the file full: wait...\nInnoDB: Doublewrite buffer not found: creating new\nInnoDB: Doublewrite buffer created\nInnoDB: 127 rollback segment(s) active.\n\nAfter this, the original tables are not accessible.\n\nmysql> use sbtest;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> show tables;\n+------------------+\n| Tables_in_sbtest |\n+------------------+\n| sbtest           |\n+------------------+\n1 row in set (0.00 sec)\n\nmysql> show create table sbtest;\nERROR 1146 (42S02): Table 'sbtest.sbtest' doesn't exist\nmysql> Bye\n[root@ip-10-245-85-116 mysql]# tail ip-10-245-85-116.err\n120629  9:46:03 [ERROR] Cannot find or open table sbtest/sbtest from\nthe internal data dictionary of InnoDB though the .frm file for the\ntable exists. Maybe you have deleted and recreated InnoDB data\nfiles but have forgotten to delete the corresponding .frm files\nof InnoDB tables, or you have moved .frm files to another database?\nor, the table contains indexes that this version of the engine\ndoesn't support.\nSee http://dev.mysql.com/doc/refman/5.5/en/innodb-troubleshooting.html\nhow you can resolve the problem.\n\nAccording to the cluster the node is up and should be able to serve data.\n\nHow to repeat:\n- Start an xtradb cluster node with innodb_data_home_dir and innodb_log_group_home_dir pointing to a different location then the datadir.\n- Add an other node to the cluster with the same configuration.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1019288/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "XtraBackup SST doesn't work properly if innodb_data_home_dir and/or innodb_log_group_home_dir is set in my.cnf", 
"title": "Bug #1019288 in Percona XtraDB Cluster: \"XtraBackup SST doesn't work properly if innodb_data_home_dir and/or innodb_log_group_home_dir is set in my.cnf\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1019288", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1019288", 
"milestone_link": null, 
"http_etag": "\"4f9983b7e6d6befd063ac2349532dbadf7e4c6ad-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~pboros", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1019288", 
"date_created": "2012-06-29T13:58:08.948000+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1019473, 
"date_in_progress": null, 
"bug_description": "For some days now we have two (out of 4) nodes crash at exactly the same time with the following error messages in the log.\n\nRecovery is only possible with a full SST.\n\n\n-- snip --\n\n-- snip --\n\ntoday:\n\nSlave SQL: Could not execute Delete_rows event on table somedb.some_table_one; Can't find record in 'some_table_one', Error_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's master log FIRST, end_log_pos 2724, Error_code: 1032\n120630  1:30:10 [Warning] WSREP: RBR event 4 Delete_rows apply warning: 120, 30131914\n120630  1:30:10 [ERROR] WSREP: Failed to apply trx: source: f879462c-c10d-11e1-0800-1f65fede6e7d version: 2 local: 0 state: CERTIFYING flags: 1 conn_id: 8664969 trx_id: 3980374692 seqnos (l: 4549962, g: 30131914, s: 30131911, d: 30131862, ts: 1341012609977406000)\n\n\nthursday:\n\nbox1:\n\n120628 8:51:06 [ERROR] Slave SQL: Could not execute Delete_rows event on\ntable somedb.some_table_two; Can't find record in 'some_table_two',\nError_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's master\nlog FIRST, end_log_pos 398, Error_code: 1032\n120628 8:51:06 [Warning] WSREP: RBR event 4 Delete_rows apply warning:\n120, 25220082\n120628 8:51:06 [ERROR] WSREP: Failed to apply trx: source:\n1337a766-b992-11e1-0800-185c16a204a2 version: 2 local: 0 state:\nCERTIFYING flags: 1 conn_id: 52899913 trx_id: 3397040790 seqnos (l:\n17019008, g: 25220082, s: 25220081, d: 25220080, ts: 1340866265202857000)\n120628 8:51:06 [ERROR] WSREP: Failed to apply app buffer:\n\u00c3\u00c3\u00be\u00c3\u00abO^S\u00c3<8a>^A, seqno: 25220082, status: WSREP_FATAL\nat galera/src/replicator_smm.cpp:apply_wscoll():50\nat galera/src/replicator_smm.cpp:apply_trx_ws():121\n120628 8:51:06 [ERROR] WSREP: Node consistency compromized, aborting...\n120628 8:51:06 [Note] WSREP: Closing send monitor...\n120628 8:51:06 [Note] WSREP: Closed send monitor.\n\nbox2:\n\n120628 8:51:06 [ERROR] Slave SQL: Could not execute Delete_rows event on\ntable somedb.some_table_two; Can't find record in 'some_table_two',\nError_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's master\nlog FIRST, end_log_pos 9845, Error_code: 1032\n120628 8:51:06 [Warning] WSREP: RBR event 10 Delete_rows apply warning:\n120, 25220082\n120628 8:51:06 [ERROR] WSREP: Failed to apply trx: source:\n1337a766-b992-11e1-0800-185c16a204a2 version: 2 local: 0 state:\nCERTIFYING flags: 1 conn_id: 52899913 trx_id: 3397040790 seqnos (l:\n17537022, g: 25220082, s: 25220081, d: 25220080, ts: 1340866265202857000)\n120628 8:51:06 [ERROR] WSREP: Failed to apply app buffer:\n\u00c3\u00c3\u00be\u00c3\u00abO^S\u00c3<8a>^A, seqno: 25220082, status: WSREP_FATAL\nat galera/src/replicator_smm.cpp:apply_wscoll():50\nat galera/src/replicator_smm.cpp:apply_trx_ws():121\n120628 8:51:06 [ERROR] WSREP: Node consistency compromized, aborting...\n120628 8:51:06 [Note] WSREP: Closing send monitor...\n120628 8:51:06 [Note] WSREP: Closed send monitor.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1019473/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Simultanious Crash of two nodes", 
"title": "Bug #1019473 in Percona XtraDB Cluster: \"Simultanious Crash of two nodes\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1019473", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1019473", 
"milestone_link": null, 
"http_etag": "\"69434fbd77ae57495e8165a2fc257f6ce121cb7d-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~frederik-kraus", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1019473", 
"date_created": "2012-06-30T02:08:02.050790+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1022250, 
"date_in_progress": null, 
"bug_description": "As reported by Patrick Zoblisein:\n\nI've been doing some more testing of the `SET GLOBAL wsrep_cluster_address='<>;` functionality and I'm still able to hang my node - regardless of whether it's changing the cluster_address to another node in the cluster, or removing the node from the cluster via gcomm://;\n\nPerforming the dynamic change in my lab environment works - no issues as Seppo notes above.  It's not until I make a connection to the node, \"post-set-GLOBAL-wsrep_cluster_address\" - here is a lot of output showing no connections (other than my working one) while I bounce back and forth between cluster_address':\n\n[root@lab1 pxc]# mysql -u root -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 4\nServer version: 5.5.20-beta23.4-log Percona XtraDB Cluster (GPL) 5.5.20-beta23.4, Revision 3724, wsrep_23.4.r3724\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the buffer.\n\nmysql> show processlist;\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n| Id | User            | Host      | db   | Command | Time | State                  | Info             | Rows_sent | Rows_examined | Rows_read |\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n|  1 | system user     |           | NULL | Sleep   |   13 | wsrep aborter idle     | NULL             |         0 |             0 |         1 |\n|  2 | system user     |           | NULL | Sleep   |   13 | NULL                   | NULL             |         0 |             0 |         1 |\n|  3 | event_scheduler | localhost | NULL | Daemon  |   13 | Waiting on empty queue | NULL             |         0 |             0 |         1 |\n|  4 | root            | localhost | NULL | Query   |    0 | sleeping               | show processlist |         0 |             0 |         1 |\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n4 rows in set (0.00 sec)\n\nmysql> show variables like 'wsrep_cluster_address';\n+-----------------------+----------------------------+\n| Variable_name         | Value                      |\n+-----------------------+----------------------------+\n| wsrep_cluster_address | gcomm://1.2.3.4:1260 |\n+-----------------------+----------------------------+\n1 row in set (0.00 sec)\n\nmysql> show processlist;\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n| Id | User            | Host      | db   | Command | Time | State                  | Info             | Rows_sent | Rows_examined | Rows_read |\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n|  1 | system user     |           | NULL | Sleep   |   38 | wsrep aborter idle     | NULL             |         0 |             0 |         1 |\n|  2 | system user     |           | NULL | Sleep   |   38 | NULL                   | NULL             |         0 |             0 |         1 |\n|  3 | event_scheduler | localhost | NULL | Daemon  |   38 | Waiting on empty queue | NULL             |         0 |             0 |         1 |\n|  4 | root            | localhost | NULL | Query   |    0 | sleeping               | show processlist |         0 |             0 |         2 |\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n4 rows in set (0.00 sec)\n\nmysql> set global wsrep_cluster_address='gcomm://';\nQuery OK, 0 rows affected (2.01 sec)\n\nmysql> show processlist;\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n| Id | User            | Host      | db   | Command | Time | State                  | Info             | Rows_sent | Rows_examined | Rows_read |\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n|  3 | event_scheduler | localhost | NULL | Daemon  |   67 | Waiting on empty queue | NULL             |         0 |             0 |         1 |\n|  4 | root            | localhost | NULL | Query   |    0 | sleeping               | show processlist |         0 |             0 |         2 |\n|  5 | system user     |           | NULL | Sleep   |    4 | wsrep aborter idle     | NULL             |         0 |             0 |         1 |\n|  6 | system user     |           | NULL | Sleep   |    4 | NULL                   | NULL             |         0 |             0 |         1 |\n+----+-----------------+-----------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n4 rows in set (0.00 sec)\n\nmysql> set global wsrep_cluster_address='gcomm://1.2.3.4:1260';\nQuery OK, 0 rows affected (3.00 sec)\n\nmysql> set global wsrep_cluster_address='gcomm://';\nQuery OK, 0 rows affected (2.00 sec)\n\nmysql> show variables like 'wsrep_cluster_address';\n+-----------------------+----------+\n| Variable_name         | Value    |\n+-----------------------+----------+\n| wsrep_cluster_address | gcomm:// |\n+-----------------------+----------+\n1 row in set (0.01 sec)\n\nmysql> set global wsrep_cluster_address='gcomm://1.2.3.5:1270';\nQuery OK, 0 rows affected (3.01 sec)\n\nmysql> show variables like 'wsrep_cluster_address';\n+-----------------------+----------------------------+\n| Variable_name         | Value                      |\n+-----------------------+----------------------------+\n| wsrep_cluster_address | gcomm://1.2.3.5:1270 |\n+-----------------------+----------------------------+\n1 row in set (0.00 sec)\n\nmysql> set global wsrep_cluster_address='gcomm://';\nQuery OK, 0 rows affected (2.00 sec)\n\nNow, I try to make another connection - which just sits there:\n[root@lab1 workarea]# mysql -u root -p\nEnter password:\n\nFlip over to my working connection and I see a new connection stuck in the \"login\" state:\nmysql> show processlist;\n+----+----------------------+-----------------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n| Id | User                 | Host            | db   | Command | Time | State                  | Info             | Rows_sent | Rows_examined | Rows_read |\n+----+----------------------+-----------------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n|  3 | event_scheduler      | localhost       | NULL | Daemon  |  810 | Waiting on empty queue | NULL             |         0 |             0 |         1 |\n|  4 | root                 | localhost       | NULL | Query   |    0 | sleeping               | show processlist |         0 |             0 |         2 |\n| 13 | system user          |                 | NULL | Sleep   |  430 | NULL                   | NULL             |         0 |             0 |         1 |\n| 14 | system user          |                 | NULL | Sleep   |  430 | wsrep aborter idle     | NULL             |         0 |             0 |         1 |\n| 15 | unauthenticated user | connecting host | NULL | Connect | NULL | login                  | NULL             |         0 |             0 |         1 |\n+----+----------------------+-----------------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n5 rows in set (0.00 sec)\n\nNOTE - this does not block all new connections by the way - open another window and make another connection and we're OK.\n[root@lab1 ~]# mysql -u root -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 16\nServer version: 5.5.20-beta23.4-log Percona XtraDB Cluster (GPL) 5.5.20-beta23.4                                                             , Revision 3724, wsrep_23.4.r3724\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the buffer.\n\nmysql> show processlist;\n+----+----------------------+-----------------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n| Id | User                 | Host            | db   | Command | Time | State                  | Info             | Rows_sent | Rows_examined | Rows_read |\n+----+----------------------+-----------------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n|  3 | event_scheduler      | localhost       | NULL | Daemon  | 1016 | Waiting on empty queue | NULL             |         0 |             0 |         1 |\n|  4 | root                 | localhost       | NULL | Sleep   |  206 |                        | NULL             |         0 |             0 |         2 |\n| 13 | system user          |                 | NULL | Sleep   |  636 | NULL                   | NULL             |         0 |             0 |         1 |\n| 14 | system user          |                 | NULL | Sleep   |  636 | wsrep aborter idle     | NULL             |         0 |             0 |         1 |\n| 15 | unauthenticated user | connecting host | NULL | Connect | NULL | login                  | NULL             |         0 |             0 |         1 |\n| 16 | root                 | localhost       | NULL | Query   |    0 | sleeping               | show processlist |         0 |             0 |         1 |\n+----+----------------------+-----------------+------+---------+------+------------------------+------------------+-----------+---------------+-----------+\n6 rows in set (0.00 sec)\n\nThe issue then becomes not being able to do another dynamic setting of the cluster_address:\n\nmysql> set global wsrep_cluster_address='1.2.3.4:1260';\n\n(wsrep_debug is equal to 1 in the cnf file)\n120329 23:48:27 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120329 23:48:27 [Note] WSREP: Assign initial position for certification: 150026, protocol version: 2\n120329 23:48:27 [Note] WSREP: Synchronized with group, ready for connections\n120329 23:48:27 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120330  0:00:03 [Note] WSREP: Stop replication\n120330  0:00:03 [Note] WSREP: Provider disconnect\n120330  0:00:03 [Note] WSREP: Closing send monitor...\n120330  0:00:03 [Note] WSREP: Closed send monitor.\n120330  0:00:03 [Note] WSREP: gcomm: terminating thread\n120330  0:00:03 [Note] WSREP: gcomm: joining thread\n120330  0:00:03 [Note] WSREP: gcomm: closing backend\n120330  0:00:03 [Note] WSREP: GMCast::handle_stable_view: view((empty))\n120330  0:00:03 [Note] WSREP: Received self-leave message.\n120330  0:00:03 [Note] WSREP: gcomm: closed\n120330  0:00:03 [Note] WSREP: Flow-control interval: [0, 0]\n120330  0:00:03 [Note] WSREP: Received SELF-LEAVE. Closing connection.\n120330  0:00:03 [Note] WSREP: Shifting SYNCED -> CLOSED (TO: 150026)\n120330  0:00:03 [Note] WSREP: RECV thread exiting 0: Success\n120330  0:00:03 [Note] WSREP: New cluster view: global state: b2a15cb4-7786-11e1-0800-0a71e5c1d29d:150026, view# -1: non-Primary, number of nodes: 0, my index: -1, protocol version 1\n120330  0:00:03 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n120330  0:00:03 [Note] WSREP: recv_thread() joined.\n120330  0:00:03 [Note] WSREP: Closing slave action queue.\n120330  0:00:03 [Note] WSREP: closing connection 15\n120330  0:00:03 [Note] WSREP: closing connection 4\n120330  0:00:03 [Note] WSREP: applier thread exiting (code:0)\n120330  0:00:03 [Note] WSREP: closing applier 13\n120330  0:00:05 [Note] WSREP: SST kill local trx: 15\n120330  0:00:05 [Note] WSREP: SST kill local trx: 4\n120330  0:00:05 [Note] WSREP: waiting for client connections to close: 5\n\nMy once working connection is no longer:\n\nmysql> show processlist;\nERROR 2006 (HY000): MySQL server has gone away\nNo connection. Trying to reconnect...\n\nPerforming a shutdown of the mysql service hangs and eventually times out - only option is to kill -9.\n\nIs there anything I can do to better troubleshoot the hung user connection?\n\nThanks\nPatrick", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1022250/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "First connection after changeing wsrep_cluster_address hangs", 
"title": "Bug #1022250 in Percona XtraDB Cluster: \"First connection after changeing wsrep_cluster_address hangs\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1022250", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1022250", 
"milestone_link": null, 
"http_etag": "\"42f4f3e746ce15eccb85ee603d09a5d32add6b44-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~ayurchen", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1022250", 
"date_created": "2012-07-08T11:32:19.738079+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1026771, 
"date_in_progress": null, 
"bug_description": "When the socket variable is defined in /etc/my.cnf in the [mysqld] section on Ubuntu 11.10 and 12.04, even when set to the location which Percona defaults to when left undefined, the startup/shutdown via init.d breaks.  Consequently if you have a my.cnf with this variable defined in place before you've done the install, the install process will fail to complete correctly.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1026771/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "defining socket in my.cnf breaks startup/shutdown on Ubuntu", 
"title": "Bug #1026771 in Percona XtraDB Cluster: \"defining socket in my.cnf breaks startup/shutdown on Ubuntu\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1026771", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1026771", 
"milestone_link": null, 
"http_etag": "\"1d2a16bf526fd13a217a56066a4e18d480dc2d87-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~stl-f", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1026771", 
"date_created": "2012-07-19T19:31:46.446425+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": "https://api.launchpad.net/devel/~ignacio-nin", 
"date_triaged": null, 
"bug_id": 1027111, 
"date_in_progress": null, 
"bug_description": "On a clean Ubuntu 12.04 (Precise) VM, I'm installing xtradb cluster for testing purposes via the percona-chef scripts. \n\nIt worked perfectly for the last couple of weeks, since this morning (20/07/12) I get the following error msg when issuing sudo apt-get update\n\n: Failed to fetch gzip:/var/lib/apt/lists/partial/repo.percona.com_apt_dists_precise_main_binary-i386_Packages  Hash Sum mismatch\n\nE: Some index files failed to download. They have been ignored, or old ones used instead.\n\nThis unfortunately halts the chef run.\n\nI saw that this morning the packages in the percona repository were updated (http://repo.percona.com/apt/dists/precise/main/binary-i386/) can it be that something went wrong in doing so?\n\nThanks!\nNick.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1027111/related_tasks", 
"date_assigned": "2012-07-23T03:26:17.392227+00:00", 
"bug_information_type": "Public", 
"bug_title": "Cannot install xtradb cluster from packages on Precise", 
"title": "Bug #1027111 in Percona XtraDB Cluster: \"Cannot install xtradb cluster from packages on Precise\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1027111", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1027111", 
"milestone_link": null, 
"http_etag": "\"72f35ef816e7b33219559f145defae47da3f9d67-5821ea45e073cfa50e0e503f11dbee96b24f1ed1\"", 
"owner_link": "https://api.launchpad.net/devel/~nboucart", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1027111", 
"date_created": "2012-07-20T14:51:29.532757+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1028813, 
"date_in_progress": null, 
"bug_description": "It seems that changing pc.ignore_quorum or pc.ignore_sb at runtime works only once and it doesn't show up the change in the variable wsrep_provider_options.\n\nHow to reproduce:\n\n2 nodes, galera replication working fine\n\nin my.cnf nothing related to the two wsrep_provider_options above.\n\non node1:\n\n\npercona1 mysql> show global variables like 'wsrep_provider_options'\\G\n*************************** 1. row ***************************\nVariable_name: wsrep_provider_options\n        Value: base_host = 10.0.2.15; base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 10.0.2.15; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = false; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n\n\nWe can see that pc.ignore_sb = false.\n\npercona1 mysql> set global wsrep_provider_options=\"pc.ignore_sb=true\";\nQuery OK, 0 rows affected (0.01 sec)\n\npercona1 mysql> show global variables like 'wsrep_provider_options'\\G\n*************************** 1. row ***************************\nVariable_name: wsrep_provider_options\n        Value: base_host = 10.0.2.15; base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 10.0.2.15; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = false; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n1 row in set (0.00 sec)\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 2     |\n+--------------------+-------+\n1 row in set (0.00 sec)\n\nSo here we have the first problem, I changed the setting, no error or warning returned and when I check the value from the variables it's cleary set to false.\n\nSo if my setting was changed, if I break the communication between the two nodes, the nodes should still accept queries, if the setting is the one showed in the variables it should fail.\n\n[root@percona2 ~]# iptables -A INPUT -d 192.168.70.3 -s 192.168.70.2 -j REJECT\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 1     |\n+--------------------+-------+\n1 row in set (0.00 sec)\n\n\npercona1 mysql> select count(*) from percona;\n+----------+\n| count(*) |\n+----------+\n|       17 |\n+----------+\n1 row in set (0.01 sec)\n\npercona1 mysql> insert into percona values (0,'percona1','baron');\nQuery OK, 1 row affected (7.60 sec)\n\n\nSo the change was done ! And the wrong value was in wsrep_provider_options\n\nNow let's change it again (meanwhile I removed the firewall rule and restarted the second node):\n\npercona1 mysql> set global wsrep_provider_options=\"pc.ignore_sb=false\";\nQuery OK, 0 rows affected (0.00 sec)\n\npercona1 mysql> show global variables like 'wsrep_provider_options'\\G\n*************************** 1. row ***************************\nVariable_name: wsrep_provider_options\n        Value: base_host = 10.0.2.15; base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 10.0.2.15; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = false; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n1 row in set (0.01 sec)\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 2     |\n+--------------------+-------+\n1 row in set (0.01 sec)\n\n\nLet's stop again the connection:\n\n[root@percona2 mysql]# iptables -A INPUT -d 192.168.70.3 -s 192.168.70.2 -j REJECT\n\npercona1 mysql> select count(*) from percona;\n+----------+\n| count(*) |\n+----------+\n|       18 |\n+----------+\n1 row in set (0.01 sec)\n\npercona1 mysql> insert into percona values (0,'percona1','fred');\nQuery OK, 1 row affected (0.01 sec)\n\n\nSo this time the change wasn't taken in consideration !\n\nLet's try again with the change in my.cnf:\n\nwsrep_provider_options = \"pc.ignore_sb = true\"\n\n\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 2     |\n+--------------------+-------+\n1 row in set (0.01 sec)\n\npercona1 mysql> show global variables like 'wsrep_provider_options'\\G\n*************************** 1. row ***************************\nVariable_name: wsrep_provider_options\n        Value: base_host = 10.0.2.15; base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 192.168.70.2; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = true; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n1 row in set (0.00 sec)\n\nWe can see that pc.ignore_sb = true;\n\nSo if I block the communication between the two peers now it should still answer queries:\n\npercona1 mysql> select count(*) from percona;\n+----------+\n| count(*) |\n+----------+\n|       19 |\n+----------+\n1 row in set (0.00 sec)\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 1     |\n+--------------------+-------+\n1 row in set (0.01 sec)\n\nNow restore the communication and add again the second node:\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 2     |\n+--------------------+-------+\n1 row in set (0.00 sec)\n\n\nChange the setting during run time:\n\npercona1 mysql> set global wsrep_provider_options=\"pc.ignore_sb=false\";\nQuery OK, 0 rows affected (0.00 sec)\n\npercona1 mysql> show global variables like 'wsrep_provider_options'\\G\n*************************** 1. row ***************************\nVariable_name: wsrep_provider_options\n        Value: base_host = 10.0.2.15; base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 192.168.70.2; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = true; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n1 row in set (0.00 sec)\n\n\nNo change in wsrep_provider_options.\n\nAnd stop communication again between nodes:\n\n| wsrep_cluster_size         | 1                                    |\n| wsrep_cluster_state_uuid   | e20f9da7-d509-11e1-0800-013f68429ec1 |\n| wsrep_cluster_status       | non-Primary                          |\n| wsrep_connected            | ON                                   |\n| wsrep_local_index          | 0                                    |\n| wsrep_provider_name        | Galera                               |\n| wsrep_provider_vendor      | Codership Oy <email address hidden>    |\n| wsrep_provider_version     | 2.1(r113)                            |\n| wsrep_ready                | OFF                                  |\n\n\npercona1 mysql> select count(*) from percona;\nERROR 1047 (08S01): Unknown command\n\n\nRestore the connection between the nodes.\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 2     |\n+--------------------+-------+\n1 row in set (0.00 sec)\n\npercona1 mysql> select count(*) from percona;\n+----------+\n| count(*) |\n+----------+\n|       19 |\n+----------+\n1 row in set (0.01 sec)\n\n\nChange for the second time pc.ignore_sb:\n\npercona1 mysql> set global wsrep_provider_options=\"pc.ignore_sb=true\";\nQuery OK, 0 rows affected (0.00 sec)\n\nand stop communication.\n\npercona1 mysql> select count(*) from percona;\nERROR 1047 (08S01): Unknown command\n\n\nSo the second change didn't work.\n\n\nLast test:\n\ncluster is started with pc.ignore_sb=true:\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 2     |\n+--------------------+-------+\n1 row in set (0.01 sec)\n\npercona1 mysql> show global variables like 'wsrep_provider_options'\\G\n*************************** 1. row ***************************\nVariable_name: wsrep_provider_options\n        Value: base_host = 10.0.2.15; base_port = 4567; evs.debug_log_mask = 0x1; evs.inactive_check_period = PT0.5S; evs.inactive_timeout = PT15S; evs.info_log_mask = 0; evs.install_timeout = PT15S; evs.join_retrans_period = PT0.3S; evs.keepalive_period = PT1S; evs.max_install_timeouts = 1; evs.send_window = 4; evs.stats_report_period = PT1M; evs.suspect_timeout = PT5S; evs.use_aggregate = true; evs.user_send_window = 2; evs.version = 0; evs.view_forget_timeout = PT5M; gcache.dir = /var/lib/mysql/; gcache.keep_pages_size = 0; gcache.mem_size = 0; gcache.name = /var/lib/mysql//galera.cache; gcache.page_size = 128M; gcache.size = 128M; gcs.fc_debug = 0; gcs.fc_factor = 0.5; gcs.fc_limit = 16; gcs.fc_master_slave = NO; gcs.max_packet_size = 64500; gcs.max_throttle = 0.25; gcs.recv_q_hard_limit = 9223372036854775807; gcs.recv_q_soft_limit = 0.25; gcs.sync_donor = NO; gmcast.listen_addr = tcp://0.0.0.0:4567; gmcast.mcast_addr = ; gmcast.mcast_ttl = 1; gmcast.peer_timeout = PT3S; gmcast.time_wait = PT5S; gmcast.version = 0; ist.recv_addr = 192.168.70.2; pc.checksum = true; pc.ignore_quorum = false; pc.ignore_sb = true; pc.linger = PT2S; pc.npvo = false; pc.version = 0; protonet.backend = asio; protonet.version = 0; replicator.causal_read_timeout = PT30S; replicator.commit_order = 3\n1 row in set (0.01 sec)\n\n\nLet's change it... twice:\n\npercona1 mysql> set global wsrep_provider_options=\"pc.ignore_sb=false\";\nQuery OK, 0 rows affected (0.00 sec)\n\npercona1 mysql> set global wsrep_provider_options=\"pc.ignore_sb=true\";\nQuery OK, 0 rows affected (0.00 sec)\n\n\nSo it should be true, let's stop communication:\n\npercona1 mysql> show global status like 'wsrep_cluster_size';\n+--------------------+-------+\n| Variable_name      | Value |\n+--------------------+-------+\n| wsrep_cluster_size | 1     |\n+--------------------+-------+\n1 row in set (0.00 sec)\n\npercona1 mysql> select count(*) from percona;\nERROR 1047 (08S01): Unknown command\n\n\nSo it's allowed to be changed only once... this is the same for pc.ignore_quorum.\n\nBut it's very weird that the values in wsrep_provider_options doesn't reflect the running settings of the cluster.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1028813/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Setting pc.ignore_quorum or pc.ignore_sb at runtime doesn't work properly", 
"title": "Bug #1028813 in Percona XtraDB Cluster: \"Setting pc.ignore_quorum or pc.ignore_sb at runtime doesn't work properly\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1028813", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1028813", 
"milestone_link": null, 
"http_etag": "\"e25165a6aae7f5e1e4afe01b10c4afd7c2480aeb-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~lefred", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1028813", 
"date_created": "2012-07-25T09:05:18.133719+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1031289, 
"date_in_progress": null, 
"bug_description": "If you shutdown a mysql node, the clustercheck script does not properly catch the error. The output is as follows (note the first line) and because of this, haproxy will not interpret the node as being down.\n\nolaf.vanzandwijk@nl14s0021:~$ /usr/bin/clustercheck \nERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)\nHTTP/1.1 503 Service Unavailable\n...\n\nRedirecting the stderr output of the mysql command to /dev/null will fix this:\n\nReplace\nWSSREP_STATUS=`mysql --host=$MYSQL_HOST --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD -e \"show status like 'wsrep_local_state';\" | awk '{if (NR!=1...\n\nwith\nWSSREP_STATUS=`mysql --host=$MYSQL_HOST --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD -e \"show status like 'wsrep_local_state';\" 2>/dev/null | awk '{if (NR!=1...", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1031289/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Error in sample clustercheck script", 
"title": "Bug #1031289 in Percona XtraDB Cluster: \"Error in sample clustercheck script\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1031289", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1031289", 
"milestone_link": null, 
"http_etag": "\"beb89a56324d7db522cde9f6c343fcfbd638ff98-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~olafz", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1031289", 
"date_created": "2012-07-31T10:13:05.873066+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-09-27T07:11:50.783288+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-09-27T07:11:50.783288+00:00", 
"bug_id": 1031292, 
"date_in_progress": "2012-09-27T07:11:50.783288+00:00", 
"bug_description": "The sample clustercheck script (Debian) contains the following line twice:\n\n/bin/echo -e \"Content-Type: Content-Type: text/plain\\r\\n\" \n\nObviously, the Content-Type is duplicate.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1031292/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Duplicate Content-Type in sample clustercheck", 
"title": "Bug #1031292 in Percona XtraDB Cluster: \"Duplicate Content-Type in sample clustercheck\"", 
"date_confirmed": "2012-09-27T07:11:50.783288+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1031292", 
"date_left_new": "2012-09-27T07:11:50.783288+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1031292", 
"milestone_link": null, 
"http_etag": "\"01d182e079446fb36177e202ce7d92f6fb778f3f-b26593bf35e31c666268fbf5c810a55ca29d9340\"", 
"owner_link": "https://api.launchpad.net/devel/~olafz", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-09-27T07:11:50.783288+00:00", 
"date_fix_released": "2012-09-27T07:11:50.783288+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1031292", 
"date_created": "2012-07-31T10:19:12.039817+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1032282, 
"date_in_progress": null, 
"bug_description": "mysqld fail to start after fresh install of percona-xtradb cluster on Ubuntu 12.04(LTS) via apt.\n\nLog message:\n120802 20:08:21 [ERROR] WSREP: wsrep_sst_method is set to 'mysqldump' yet mysqld bind_address is set to '127.0.0.1', which makes it impossible to receive state transfer from another node, since mysqld won't accept such connections. If you wish to use mysqldump state transfer method, s\n120802 20:08:21 [ERROR] Aborting\n\nDefault bind_address is set to 127.0.0.1 and due to https://bugs.launchpad.net/codership-mysql/+bug/800138, mysql refuse to start. Even if you  add  bind_address = <some ip>, mysqld refuse start until you remove \"127.0.0.1\".", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1032282/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "error installing xtradb-cluster on Ubuntu(mysqld refuse to start)", 
"title": "Bug #1032282 in Percona XtraDB Cluster: \"error installing xtradb-cluster on Ubuntu(mysqld refuse to start)\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1032282", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1032282", 
"milestone_link": null, 
"http_etag": "\"e7348268cf7eb9e677b17c26821345f320b3e937-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~tolya-msu", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1032282", 
"date_created": "2012-08-02T16:29:11.668729+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1034653, 
"date_in_progress": null, 
"bug_description": "Server Version : ubuntu 10.04.1 LTS\npercona version: 5.5.24-55-log Percona XtraDB Cluster (GPL), wsrep_23.6.r341\n\nConfiguration 3 nodes and 1 load balancer\n\n(my.cnf) in node 1 \n++++++++++++++++++++++++++++\n[mysqld_safe]\nsocket\t\t= /var/run/mysqld/mysqld.sock\nnice\t\t= 0\nwsrep_urls      = gcomm://10.1.6.118:4567,gcomm://10.1.3.30:4567,gcomm://10.1.3.101:4567,gcomm://\n\n[mysqld]\n#\n# * Basic Settings\n#\nserver_id=1\nbinlog_format=ROW   \nwsrep_provider=/usr/lib64/libgalera_smm.so   \n#wsrep_cluster_address=gcomm://\nwsrep_slave_threads=2 \nwsrep_cluster_name=dev_cluster \nwsrep_sst_method=rsync\nwsrep_node_name=node1   \ninnodb_locks_unsafe_for_binlog=1 \ninnodb_autoinc_lock_mode=2\nlog_slave_updates\nwsrep_replicate_myisam=1\nwsrep_sst_receive_address=10.1.6.118\nwsrep_provider_options = \"gmcast.listen_addr=tcp://0.0.0.0:4567; ist.recv_addr=10.1.6.118:4568; \"\n++++++++++++++++++++++++++++\n\n(my.cnf) in node 2 \n++++++++++++++++++++++++++++\n# This was formally known as [safe_mysqld]. Both versions are currently parsed.\n[mysqld_safe]\nsocket\t\t= /var/run/mysqld/mysqld.sock\nnice\t\t= 0\nwsrep_urls      = gcomm://10.1.6.118:4567,gcomm://10.1.3.30:4567,gcomm://10.1.3.101:4567,gcomm://\n\n[mysqld]\n#\n# * Basic Settings\n#\nserver_id=2\nbinlog_format=ROW   \nwsrep_provider=/usr/lib64/libgalera_smm.so   \n#wsrep_cluster_address=gcomm://10.1.6.118:4567\nwsrep_slave_threads=2 \nwsrep_cluster_name=dev_cluster \nwsrep_sst_method=rsync\nwsrep_node_name=node2   \ninnodb_locks_unsafe_for_binlog=1 \ninnodb_autoinc_lock_mode=2\nlog_slave_updates\nwsrep_replicate_myisam=1\nwsrep_sst_receive_address=10.1.3.30\nwsrep_provider_options = \"gmcast.listen_addr=tcp://0.0.0.0:4567; ist.recv_addr=10.1.3.30:4568; \"\n++++++++++++++++++++++++++++\n\n(my.cnf) in node 3\n\n++++++++++++++++++++++++++++\n# This was formally known as [safe_mysqld]. Both versions are currently parsed.\n[mysqld_safe]\nsocket\t\t= /var/run/mysqld/mysqld.sock\nnice\t\t= 0\nwsrep_urls      = gcomm://10.1.6.118:4567,gcomm://10.1.3.30:4567,gcomm://10.1.3.101:4567,gcomm://\n\n[mysqld]\n#\n# * Basic Settings\n#\nserver_id=3\nbinlog_format=ROW\nwsrep_provider=/usr/lib64/libgalera_smm.so\n#wsrep_cluster_address=gcomm://10.1.6.118:4567\nwsrep_slave_threads=2\nwsrep_cluster_name=dev_cluster\nwsrep_sst_method=rsync\nwsrep_node_name=node3\ninnodb_locks_unsafe_for_binlog=1\ninnodb_autoinc_lock_mode=2\nlog_slave_updates\nwsrep_replicate_myisam=1\nwsrep_sst_receive_address=10.1.3.101\nwsrep_provider_options = \"gmcast.listen_addr=tcp://0.0.0.0:4567; ist.recv_addr=10.1.3.101:4568; \"\n++++++++++++++++++++++++++++\n\n\nwhen any of the nodes is rebooted the cluster does not start on that node\nand the database has to be stopped using \nsudo service mysql stop\nand then \nstarted using\nsudo /etc/init.d/mysql start\n\nonce this is done, then the cluster starts fine..\nmy question is what is the difference in those 2 commands ?\nis the mysqld_safe entries causing this , how do i fix this issue?\n\nPlease let me know", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1034653/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Percona node will not auto start on reboot", 
"title": "Bug #1034653 in Percona XtraDB Cluster: \"Percona node will not auto start on reboot\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1034653", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1034653", 
"milestone_link": null, 
"http_etag": "\"f15e975b5710e75da08029bf27aa8be667a9e4d5-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~ajkedar", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1034653", 
"date_created": "2012-08-08T22:18:43.670777+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1034995, 
"date_in_progress": null, 
"bug_description": "Hello,\n\nI have installed percona xtradb cluster latest 5.5 version and I got an xtrabackup dump from another active DB which was a node of another cluster in the past but not anymore.\nThe dump works and mysql starts fine without wsrep on.\nWhen I turn it on mysql cycles trying to start forever and I get the following error in the log.\n\n120809 18:17:57 [Note] WSREP: gcomm: connected\n120809 18:17:57 [Note] WSREP: Changing maximum packet size to 64500, resulting msg size: 32636\n120809 18:17:57 [Note] WSREP: Shifting CLOSED -> OPEN (TO: 0)\n120809 18:17:57 [Note] WSREP: Opened channel 'front_data'\n120809 18:17:57 [Note] WSREP: New COMPONENT: primary = yes, bootstrap = no, my_idx = 0, memb_num = 1\n120809 18:17:57 [Note] /usr/sbin/mysqld: ready for connections.\nVersion: '5.5.24-55-log'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  Percona XtraDB Cluster (GPL), wsrep_23.6.r341\n120809 18:17:57 [Note] WSREP: Starting new group from scratch: c8442ba0-e23d-11e1-0800-db2092f6eba3\n120809 18:17:57 [Note] WSREP: STATE_EXCHANGE: sent state UUID: c84449f1-e23d-11e1-0800-2f6508cbc19b\n120809 18:17:57 [Note] WSREP: STATE EXCHANGE: sent state msg: c84449f1-e23d-11e1-0800-2f6508cbc19b\n120809 18:17:57 [Note] WSREP: STATE EXCHANGE: got state msg: c84449f1-e23d-11e1-0800-2f6508cbc19b from 0 (data3)\n120809 18:17:57 [Note] WSREP: Quorum results:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0version    = 2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0component  = PRIMARY,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0conf_id    = 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0members    = 1/1 (joined/total),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0act_id     = 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0last_appl. = -1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0protocols  = 0/4/2 (gcs/repl/appl),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0group UUID = c8442ba0-e23d-11e1-0800-db2092f6eba3\n120809 18:17:57 [Note] WSREP: Flow-control interval: [8, 16]\n120809 18:17:57 [Note] WSREP: Restored state OPEN -> JOINED (0)\n120809 18:17:57 [Note] WSREP: Member 0 (data3) synced with group.\n120809 18:17:57 [Note] WSREP: Shifting JOINED -> SYNCED (TO: 0)\n120809 18:17:57 [Note] WSREP: New cluster view: global state: c8442ba0-e23d-11e1-0800-db2092f6eba3:0, view# 1: Primary, number of nodes: 1, my index: 0, protocol version 2\n120809 18:17:57 [ERROR] WSREP: Undetected state gap. Can't continue.\n120809 18:17:57 [ERROR] WSREP: WSREP: Group state: c8442ba0-e23d-11e1-0800-db2092f6eba3:0\n120809 18:17:57 [ERROR] WSREP: WSREP: Local state: db7c6ef3-e1a4-11e1-0800-fb749cdd1cff:-1\n16:17:57 UTC - mysqld got signal 6 ;\nThis could be because you hit a bug. It is also possible that this binary\nor one of the libraries it was linked against is corrupt, improperly built,\nor misconfigured. This error can also be caused by malfunctioning hardware.\nWe will try our best to scrape up some info that will hopefully help\ndiagnose the problem, but since we have already crashed,\nsomething is definitely wrong and this may fail.\nPlease help us make Percona Server better by reporting any\nbugs at https://bugs.launchpad.net/percona-server\n\nkey_buffer_size=1048576\nread_buffer_size=67108864\nmax_used_connections=0\nmax_threads=200\nthread_count=17\nconnection_count=17\nIt is possible that mysqld could use up to\nkey_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 117968349 K  bytes of memory\nHope that's ok; if not, decrease some variables in the equation.\n\nThread pointer: 0x1f2f38c0\nAttempting backtrace. You can use the following information to find out\nwhere mysqld died. If you see no messages after this, something went\nterribly wrong...\nstack_bottom = 7f1cf16eeea8 thread_stack 0x40000\n/usr/sbin/mysqld(my_print_stacktrace+0x35)[0x7dea25]\n/usr/sbin/mysqld(handle_fatal_signal+0x4a4)[0x6ab794]\n/lib/libpthread.so.0(+0xeff0)[0x7f217e70fff0]\n/lib/libc.so.6(gsignal+0x35)[0x7f217d90b1b5]\n/lib/libc.so.6(abort+0x180)[0x7f217d90dfc0]\n/usr/sbin/mysqld[0x665f17]\n/usr/lib64/libgalera_smm.so(_ZN6galera13ReplicatorSMM19process_conf_changeEPvRK15wsrep_view_infoiNS_10Replicator5StateEl+0x23d)[0x7f1cf3c03add]\n/usr/lib64/libgalera_smm.so(_ZN6galera15GcsActionSource8dispatchEPvRK10gcs_action+0x8ee)[0x7f1cf3bdf68e]\n/usr/lib64/libgalera_smm.so(_ZN6galera15GcsActionSource7processEPv+0x58)[0x7f1cf3bdf948]\n/usr/lib64/libgalera_smm.so(_ZN6galera13ReplicatorSMM10async_recvEPv+0xfd)[0x7f1cf3c0266d]\n/usr/lib64/libgalera_smm.so(galera_recv+0x23)[0x7f1cf3c1c1b3]\n/usr/sbin/mysqld(_Z25wsrep_replication_processP3THD+0x6b)[0x59d44b]\n/usr/sbin/mysqld(start_wsrep_THD+0x3f3)[0x52bad3]\n/lib/libpthread.so.0(+0x68ca)[0x7f217e7078ca]\n/lib/libc.so.6(clone+0x6d)[0x7f217d9a892d]\n\nTrying to get some variables.\nSome pointers may be invalid and cause the dump to abort.\nQuery (0): is an invalid pointer\nConnection ID (thread ID): 2\nStatus: NOT_KILLED\n\nI am running debian 6.0 amd64 and here is my config for wsrep:\n\n[mysqld_safe]\n\n# GALERA CONFIGURATION\n\nwsrep_urls = gcomm://\n\n[mysqld]\n\nwsrep_provider=/usr/lib64/libgalera_smm.so\nwsrep_cluster_name=data_cluster\nwsrep_node_name=node1\nwsrep_provider_options=\"gcache.size=1G;\"\nwsrep_node_address=192.168.1.6\nwsrep_sst_method=skip\nwsrep_slave_threads=16\nwsrep_sst_receive_address=192.168.1.6\n\nlog-bin=/vol0/mysql/mysql-bin\nexpire_logs_days=5\nbinlog_format=row", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1034995/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "xtradb new cluster setup with xtrabackup snapshot crashes", 
"title": "Bug #1034995 in Percona XtraDB Cluster: \"xtradb new cluster setup with xtrabackup snapshot crashes\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1034995", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1034995", 
"milestone_link": null, 
"http_etag": "\"889ba6d5576abd5b779e6fa57c6d32698ca17580-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~d-ilias", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1034995", 
"date_created": "2012-08-09T17:18:54.004787+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1035927, 
"date_in_progress": null, 
"bug_description": "When the mysql server on a node is hung clustercheck will also hang and never fail the server over.\n\nSteps to reproduce:\n\n1) setup a 3 node cluster\n2) configure ha_proxy to run with an active writer and 2 backup servers\n3) start a sysbench test \n4) log into the writer node and run 'kill -SIGSTOP <mysqld_pid>' as root to hang the server\n\nSetup details can be found here: http://www.mysqlperformanceblog.com/2012/06/20/percona-xtradb-cluster-reference-architecture-with-haproxy/\n\nThe sysbench test should timeout and the ha_proxy writer vip should also hang.\n\nAttached is a python version of clustercheck that will timeout after 30 seconds and mark the node as failed.  I didn't create a branch since this would add a new python dependency to PXC so, I wasn't sure if this needed to be written in perl or not in order to be accepted.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1035927/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "clustercheck hangs when server is hung", 
"title": "Bug #1035927 in Percona XtraDB Cluster: \"clustercheck hangs when server is hung\"", 
"date_confirmed": "2012-09-27T07:31:42.294570+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1035927", 
"date_left_new": "2012-09-27T07:31:42.294570+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1035927", 
"milestone_link": null, 
"http_etag": "\"30e908643683c70390ce10f43579907024cce5a1-a25e87a79f5bf0ce3f644a2d03f33877f55f2252\"", 
"owner_link": "https://api.launchpad.net/devel/~nakoa-mccullough", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1035927", 
"date_created": "2012-08-12T18:27:49.577402+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-09-07T16:49:02.576959+00:00", 
"status": "Fix Released", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-09-07T16:49:02.576959+00:00", 
"bug_id": 1036774, 
"date_in_progress": "2012-09-07T16:49:02.576959+00:00", 
"bug_description": "When exclusive key matches a key in certification index it should always check for shared reference (even if exclusive reference exist) and update dependency accordingly.\n\nThis bug causes some failures in regression tests for  https://bugs.launchpad.net/codership-mysql/+bug/1013978", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1036774/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "exclusive key does not depend on shared key", 
"title": "Bug #1036774 in Percona XtraDB Cluster: \"exclusive key does not depend on shared key\"", 
"date_confirmed": "2012-09-07T16:49:02.576959+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1036774", 
"date_left_new": "2012-09-07T16:49:02.576959+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1036774", 
"milestone_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+milestone/percona-xtradb-cluster-5.5.27", 
"http_etag": "\"e921d339e03dfca77fe88f3438365aa6ebb3b392-92d11bac609798ed68d286597172671c96e4bf6d\"", 
"owner_link": "https://api.launchpad.net/devel/~gm-outside+launchpad", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": "2012-09-07T16:49:02.576959+00:00", 
"date_fix_released": "2012-09-07T16:49:02.576959+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1036774", 
"date_created": "2012-09-07T00:29:14.632361+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1037380, 
"date_in_progress": null, 
"bug_description": "A developer changed a MyISAM table to InnoDB and either as a coincidence or due to, the cluster crashed at the same time with a very... odd... error:\n\n120815 19:15:34 [ERROR] WSREP: Failed to apply app buffer: \u00c3\nW,P^S^B, seqno: 15897511, status: WSREP_FATALly_wscoll():50\n         at galera/src/replicator_smm.cpp:apply_trx_ws():121\n120815 19:15:34 [ERROR] WSREP: Node consistency compromized, aborting...\n\n\nThe garbage text at the beginning of the second line was actually part of the error log. This happened on two out of three nodes and the third node on which the query was running stayed up, but had issues without being restarted. Please let me know what additional information you need.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1037380/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "WSREP Crash on MyISAM -> InnoDB", 
"title": "Bug #1037380 in Percona XtraDB Cluster: \"WSREP Crash on MyISAM -> InnoDB\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1037380", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1037380", 
"milestone_link": null, 
"http_etag": "\"3c6ccfdfe7d4ffc582334811ecf6424914dd8979-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~jlondon", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1037380", 
"date_created": "2012-08-16T02:45:03.225690+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": "2012-09-27T17:09:49.091671+00:00", 
"status": "Invalid", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1047886, 
"date_in_progress": null, 
"bug_description": "5.5.27.\n\nIf, on a DONOR node,  I add --no_lock to /usr/bin/wsrep_sst_xtrabackup INNOBACKUPEX_ARGS like this:\n\nINNOBACKUPEX_ARGS=\"--no-lock\"\n\n\nThen SST fails on the JOINER node with this error:\n\n\n120908 17:31:44 [ERROR] WSREP: Process completed with error: wsrep_sst_xtrabackup 'joiner' '192.168.70.4' '' '/var/lib/mysql/' '/etc/my.cnf' '27134' 2>sst.err: 32 (Broken pipe)\n120908 17:31:44 [ERROR] WSREP: Failed to parse uuid:seqno pair: 'xtrabackup process ended without creating '/var/lib/mysql//xtrabackup_galera_info''\n120908 17:31:44 [ERROR] WSREP: SST failed: 22 (Invalid argument)\n120908 17:31:44 [ERROR] Aborting\n\nThere is no content in the sst.err on the JOINER.  \n\nI know adding this argument worked correctly in 5.5.24, but seems broken in 5.5.27", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1047886/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Adding --no-lock in wsrep_sst_xtrabackup causes SST to fail", 
"title": "Bug #1047886 in Percona XtraDB Cluster: \"Adding --no-lock in wsrep_sst_xtrabackup causes SST to fail\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1047886", 
"date_left_new": "2012-09-27T16:59:51.075660+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1047886", 
"milestone_link": null, 
"http_etag": "\"7543d080c18b9c181dd70a1cdefd2bdd595a5110-f7133afc7c13191d8a487e081592a5120d81cf56\"", 
"owner_link": "https://api.launchpad.net/devel/~jay-janssen", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1047886", 
"date_created": "2012-09-08T15:35:52.411530+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": true, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1049105, 
"date_in_progress": null, 
"bug_description": "Hi,\n\nI have a big problem since I update my Percona 5.5.24 to 5.5.27:\nError message in second and third node node is :\n\ninvoke-rc.d: initscript mysql, action \"start\" failed.\ndpkg: error processing percona-xtradb-cluster-server-5.5 (--configure):\nsubprocess installed post-installation script returned error exit status 1\nconfigured to not write apport reports\nErrors were encountered while processing:\npercona-xtradb-cluster-server-5.5\nE: Sub-process /usr/bin/dpkg returned an error code (1)\n\nthe error log in mysql :\n\n120911 13:57:14 [ERROR] WSREP: gcs/src/gcs_backend.c:gcs_backend_init():87: Invalid backend URI: 0\n120911 13:57:14 [ERROR] WSREP: gcs/src/gcs_core.c:gcs_core_open():202: Failed to initialize backend using '0': -22 (Invalid argument)\n120911 13:57:14 [ERROR] WSREP: gcs/src/gcs.c:gcs_open():1290: Failed to open channel 'my_wsrep_cluster' at '0': -22 (Invalid argument)\n120911 13:57:14 [ERROR] WSREP: gcs connect failed: Invalid argument\n120911 13:57:14 [ERROR] WSREP: wsrep::connect() failed: 6\n120911 13:57:14 [ERROR] Aborting\n\nAnd syslog :\n\nSep 11 13:56:44 t2-mysql02 mysqld_safe[10491]: ERROR: 1064 You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'ALTER TABLE user ADD column Show_view_priv enum('N','Y') CHARACTER SET utf8 NOT ' at line 1\nSep 11 13:56:44 t2-mysql02 mysqld_safe[10491]: 120911 13:56:44 [ERROR] Aborting\nSep 11 13:56:44 t2-mysql02 mysqld_safe[10491]:\nSep 11 13:56:44 t2-mysql02 mysqld_safe[10491]: 120911 13:56:44 [Note] WSREP: Service disconnected.\nSep 11 13:56:45 t2-mysql02 mysqld_safe[10491]: 120911 13:56:45 [Note] WSREP: Some threads may fail to exit.\nSep 11 13:56:45 t2-mysql02 mysqld_safe[10491]: 120911 13:56:45 InnoDB: Starting shutdown...\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10491]: 120911 13:56:48 InnoDB: Shutdown completed; log sequence number 2549519372\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10491]: 120911 13:56:48 [Note] /usr/sbin/mysqld: Shutdown complete\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10491]:\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 [Warning] option 'wsrep_max_ws_rows': unsigned value 67108864 adjusted to 1048576\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 [Note] Flashcache bypass: disabled\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 [Note] Flashcache setup error is : ioctl failed\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]:\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 [Note] WSREP: Read nil XID from storage engines, skipping position init\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 [Note] WSREP: wsrep_load(): loading provider library 'none'\nSep 11 13:56:48 t2-mysql02 kernel: [11361.879267] mysqld: sending ioctl 4004fecd to a partition!\nSep 11 13:56:48 t2-mysql02 kernel: [11361.879272] mysqld: sending ioctl 4004fecd to a partition!\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 [Note] Plugin 'FEDERATED' is disabled.\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: The InnoDB memory heap is disabled\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: Mutexes and rw_locks use GCC atomic builtins\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: Compressed tables use zlib 1.2.3\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: Using Linux native AIO\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: Initializing buffer pool, size = 1.0G\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: Completed initialization of buffer pool\nSep 11 13:56:48 t2-mysql02 mysqld_safe[10544]: 120911 13:56:48 InnoDB: highest supported file format is Barracuda.\nSep 11 13:56:53 t2-mysql02 mysqld_safe[10544]: 120911 13:56:53 InnoDB: Waiting for the background threads to start\nSep 11 13:56:54 t2-mysql02 mysqld_safe[10544]: 120911 13:56:54 Percona XtraDB (http://www.percona.com) 1.1.8-rel28.1 started; log sequence number 2549519372\nSep 11 13:56:54 t2-mysql02 mysqld_safe[10544]: 120911 13:56:54 [Note] WSREP: Service disconnected.\nSep 11 13:56:55 t2-mysql02 mysqld_safe[10544]: 120911 13:56:55 [Note] WSREP: Some threads may fail to exit.\nSep 11 13:56:55 t2-mysql02 mysqld_safe[10544]: 120911 13:56:55 InnoDB: Starting shutdown...\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10544]: 120911 13:56:58 InnoDB: Shutdown completed; log sequence number 2549519372\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 [Warning] option 'wsrep_max_ws_rows': unsigned value 67108864 adjusted to 1048576\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 [Note] Flashcache bypass: disabled\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 [Note] Flashcache setup error is : ioctl failed\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]:\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 [Note] WSREP: Read nil XID from storage engines, skipping position init\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 [Note] WSREP: wsrep_load(): loading provider library 'none'\nSep 11 13:56:58 t2-mysql02 kernel: [11371.684835] mysqld: sending ioctl 4004fecd to a partition!\nSep 11 13:56:58 t2-mysql02 kernel: [11371.684839] mysqld: sending ioctl 4004fecd to a partition!\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 [Note] Plugin 'FEDERATED' is disabled.\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: The InnoDB memory heap is disabled\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: Mutexes and rw_locks use GCC atomic builtins\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: Compressed tables use zlib 1.2.3\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: Using Linux native AIO\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: Initializing buffer pool, size = 1.0G\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: Completed initialization of buffer pool\nSep 11 13:56:58 t2-mysql02 mysqld_safe[10597]: 120911 13:56:58 InnoDB: highest supported file format is Barracuda.\nSep 11 13:57:02 t2-mysql02 mysqld_safe[10597]: 120911 13:57:02 InnoDB: Waiting for the background threads to start\nSep 11 13:57:03 t2-mysql02 mysqld_safe[10597]: 120911 13:57:03 Percona XtraDB (http://www.percona.com) 1.1.8-rel28.1 started; log sequence number 2549519372\nSep 11 13:57:03 t2-mysql02 mysqld_safe[10597]: ERROR: 1050 Table 'plugin' already exists\nSep 11 13:57:03 t2-mysql02 mysqld_safe[10597]: 120911 13:57:03 [ERROR] Aborting\nSep 11 13:57:03 t2-mysql02 mysqld_safe[10597]:\nSep 11 13:57:03 t2-mysql02 mysqld_safe[10597]: 120911 13:57:03 [Note] WSREP: Service disconnected.\nSep 11 13:57:04 t2-mysql02 mysqld_safe[10597]: 120911 13:57:04 [Note] WSREP: Some threads may fail to exit.\nSep 11 13:57:04 t2-mysql02 mysqld_safe[10597]: 120911 13:57:04 InnoDB: Starting shutdown...\nSep 11 13:57:07 t2-mysql02 mysqld_safe[10597]: 120911 13:57:07 InnoDB: Shutdown completed; log sequence number 2549519372\nSep 11 13:57:07 t2-mysql02 mysqld_safe[10597]: 120911 13:57:07 [Note] /usr/sbin/mysqld: Shutdown complete\n\nSep 11 14:14:37 t2-mysql02 /etc/init.d/mysql[3676]: 0 processes alive and '/usr/bin/mysqladmin --defaults-file=/etc/mysql/debian.cnf ping' resulted in\nSep 11 14:14:37 t2-mysql02 /etc/init.d/mysql[3676]: #007/usr/bin/mysqladmin: connect to server at 'localhost' failed\nSep 11 14:14:37 t2-mysql02 /etc/init.d/mysql[3676]: error: 'Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)'\nSep 11 14:14:37 t2-mysql02 /etc/init.d/mysql[3676]: Check that mysqld is running and that the socket: '/var/run/mysqld/mysqld.sock' exists!", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1049105/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Percona XtraDBCluster", 
"title": "Bug #1049105 in Percona XtraDB Cluster: \"Percona XtraDBCluster\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1049105", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1049105", 
"milestone_link": null, 
"http_etag": "\"eb8a76f627c69a54fce5717d18cd818fa7babac9-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~azigui", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1049105", 
"date_created": "2012-09-11T13:05:02.073306+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Confirmed", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1050654, 
"date_in_progress": null, 
"bug_description": "> yum install percona-toolkit\n\nLoaded plugins: changelog, fastestmirror, langpacks, presto, refresh-packagekit, security, yum-fast-downloader\nLoading mirror speeds from cached hostfile\n * fedora: fedora.iitm.ac.in\n * livna: rpm.livna.org\n * rpmfusion-free: mirror.bjtu.edu.cn\n * rpmfusion-free-updates: mirror.bjtu.edu.cn\n * rpmfusion-nonfree: mirror.bjtu.edu.cn\n * rpmfusion-nonfree-updates: mirror.bjtu.edu.cn\n * updates: fedora.iitm.ac.in\nResolving Dependencies\n--> Running transaction check\n---> Package percona-toolkit.noarch 0:2.1.3-2 will be installed\n--> Processing Dependency: perl(DBD::mysql) >= 1.0 for package: percona-toolkit-2.1.3-2.noarch\n--> Running transaction check\n---> Package perl-DBD-MySQL.x86_64 0:4.020-2.fc17 will be installed\n--> Processing Dependency: libmysqlclient.so.18(libmysqlclient_16)(64bit) for package: perl-DBD-MySQL-4.020-2.fc17.x86_64\nPackage mysql-libs is obsoleted by Percona-Server-shared-51, but obsoleting package does not provide for requirements\n--> Finished Dependency Resolution\nError: Package: perl-DBD-MySQL-4.020-2.fc17.x86_64 (fedora)\n           Requires: libmysqlclient.so.18(libmysqlclient_16)(64bit)\n           Available: mysql-libs-5.5.23-1.fc17.x86_64 (fedora)\n               libmysqlclient.so.18(libmysqlclient_16)(64bit)\n           Available: mysql-libs-5.5.27-1.fc17.x86_64 (updates)\n               libmysqlclient.so.18(libmysqlclient_16)(64bit)\n You could try using --skip-broken to work around the problem\n You could try running: rpm -Va --nofiles --nodigest\n\n======================\n\n\nThis is because: \n\nrpm -q --provides Percona-Server-shared-55-5.5.27-rel28.1.296.rhel6.x86_64\n\nlibmysqlclient.so.18()(64bit)\nlibtool(/usr/lib64/mysql/plugin/libfnv1a_udf.la)\nlibtool(/usr/lib64/mysql/plugin/libfnv_udf.la)\nlibtool(/usr/lib64/mysql/plugin/libmurmur_udf.la)\nmysql-shared\nPercona-Server-shared-55 = 5.5.27-rel28.1.296.rhel6\nPercona-Server-shared-55(x86-64) = 5.5.27-rel28.1.296.rhel6\n\n\nSo, Percona-Server-shared doesn't provide libmysqlclient.so.18(libmysqlclient_16)(64bit) which is why the build fails.\n\nIn contrast, mysql-libs (the upstream counterpart) provides following:\n\n    config(mysql-libs) = 5.5.27-1.fc17\n    libmysqlclient.so.18\n    libmysqlclient.so.18(libmysqlclient_16)\n    libmysqlclient.so.18(libmysqlclient_18)\n    mysql-libs = 5.5.27-1.fc17\n    mysql-libs(x86-32) = 5.5.27-1.fc17\n\nfrom http://pkgs.org/fedora-17/fedora-updates-i386/mysql-libs-5.5.27-1.fc17.i686.rpm.html#provides\n\n1. So, we need to update the spec file for Percona-Server-shared with \n\n    libmysqlclient.so.18(libmysqlclient_16)\n    libmysqlclient.so.18(libmysqlclient_18)\n\n2. Alternatively, we can also put mysql-libs in provides. There is no mysql-shared. This also causes problems elsewhere.\n\n\nNote, that this problem will arise in CentOS too once perl-DBD-MySQL is updated in its repos.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1050654/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "Add      libmysqlclient.so.18(libmysqlclient_16) and      libmysqlclient.so.18(libmysqlclient_18) to Provides of Percona-Server-shared-55 and Percona-XtraDB-Cluster-shared", 
"title": "Bug #1050654 in Percona XtraDB Cluster: \"Add      libmysqlclient.so.18(libmysqlclient_16) and      libmysqlclient.so.18(libmysqlclient_18) to Provides of Percona-Server-shared-55 and Percona-XtraDB-Cluster-shared\"", 
"date_confirmed": "2012-09-17T07:15:09.851382+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1050654", 
"date_left_new": "2012-09-17T07:15:09.851382+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1050654", 
"milestone_link": null, 
"http_etag": "\"44e72d4caf90ddae3d4cd36d135b8c74e6424426-a25e87a79f5bf0ce3f644a2d03f33877f55f2252\"", 
"owner_link": "https://api.launchpad.net/devel/~raghavendra-prabhu", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1050654", 
"date_created": "2012-09-17T07:14:54.623459+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"pkg"
]
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1050775, 
"date_in_progress": null, 
"bug_description": "[In:Percona XtraDB Cluster Documentation]\n\nOn page http://www.percona.com/doc/percona-xtradb-cluster/howtos/haproxy.html\n\n>To implement this setup you will need two scripts:\n>\n>       clustercheck (place to /usr/local/bin) and a config for xined and\n>        mysqlchk (place to /etc/xined.d) on each node.\n\nInstead of \"xined\" and \"/etc/xined.d\" there should be \"xinetd\" and \"/etc/xinetd.d\"", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1050775/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "[DOC] Load balancing with HAProxy typo", 
"title": "Bug #1050775 in Percona XtraDB Cluster: \"[DOC] Load balancing with HAProxy typo\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1050775", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1050775", 
"milestone_link": null, 
"http_etag": "\"0e33455a02edcb60a2466a8037c1869dc11df65a-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~p-a-lipatov", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1050775", 
"date_created": "2012-09-14T07:26:04.598407+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": [
"doc"
]
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1054319, 
"date_in_progress": null, 
"bug_description": "OS: Linux db3 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/Linux\nServer version: 5.5.27-log Percona XtraDB Cluster (GPL), wsrep_23.6.r356\n\nPercona-XtraDB-Cluster-shared-5.5.27-23.6.356.rhel6.x86_64\nPercona-XtraDB-Cluster-galera-2.0-1.114.rhel6.x86_64\nPercona-Server-shared-compat-5.5.27-rel28.1.296.rhel6.x86_64\nPercona-XtraDB-Cluster-server-5.5.27-23.6.356.rhel6.x86_64\nPercona-XtraDB-Cluster-client-5.5.27-23.6.356.rhel6.x86_64\npercona-xtrabackup-2.0.2-461.rhel6.x86_64\n\nI'm trying to setup new cluster. \nWhen joining new node sst failed on flush tables step (https://bugs.launchpad.net/percona-xtrabackup/+bug/1022874)\n\nmysql> flush tables with read lock;\nERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction\n\nError appears immediately after issuing command without delays, but all threads are in \"Waiting for global read lock\" state until \"unlock tables\"", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1054319/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "\"flush tables with read lock\" wrong error message", 
"title": "Bug #1054319 in Percona XtraDB Cluster: \"\"flush tables with read lock\" wrong error message\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1054319", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1054319", 
"milestone_link": null, 
"http_etag": "\"e9ecf6a94d89b3dfc3fbc80ac2a34992b6951c74-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~pservit", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1054319", 
"date_created": "2012-09-21T20:08:59.028815+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "Incomplete", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": "2012-09-27T11:03:37.849618+00:00", 
"bug_id": 1055547, 
"date_in_progress": "2012-09-27T11:03:37.849618+00:00", 
"bug_description": "This should be related to https://bugs.launchpad.net/percona-xtrabackup/+bug/902567.\n\n\nWhen  innodb_flush_method is set to O_DIRECT, it's not possible to use xtrabackup as SST, the error is the following :\n\n120629  6:12:16  InnoDB: Starting shutdown...\n120629  6:12:19  InnoDB: Shutdown completed; log sequence number 3351590558\nInnoDB: Error: tried to read 2048 bytes at offset 0 0.\nInnoDB: Was only able to read 0.\n120629  6:12:19  InnoDB: Operating system error number 22 in a file operation.\nInnoDB: Error number 22 means 'Invalid argument'.\nInnoDB: Some operating system error numbers are described at\nInnoDB: http://dev.mysql.com/doc/refman/5.5/en/operating-system-error-codes.html\nInnoDB: File operation call: 'read'.\nInnoDB: Cannot continue operation.\ninnobackupex: Error: \ninnobackupex: ibbackup failed at /usr//bin/innobackupex line 371.\n\n\nFor Percona internal reference check #23358\n\nWorkaround:\n---------------\n\ncopy the my.cnf and in the copy remove innodb_flush_method and force in  /usr/bin/wsrep_sst_xtrabackup, to use the copy:\n\nline 65:\n\n-CONF=$5\n+CONF=/etc/my.cnf-sst", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1055547/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "xtrabackup as SST fails when innodb_flush_method is O_DIRECT", 
"title": "Bug #1055547 in Percona XtraDB Cluster: \"xtrabackup as SST fails when innodb_flush_method is O_DIRECT\"", 
"date_confirmed": "2012-09-27T11:03:37.849618+00:00", 
"bug_link": "https://api.launchpad.net/devel/bugs/1055547", 
"date_left_new": "2012-09-27T11:03:37.849618+00:00", 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1055547", 
"milestone_link": null, 
"http_etag": "\"0f341c50e90a6775c370cbfa5de4c17d144c8aa8-d658fc37755fb0720c19af892d4b2aa6fc16e37d\"", 
"owner_link": "https://api.launchpad.net/devel/~lefred", 
"date_left_closed": null, 
"date_incomplete": "2012-09-27T11:03:37.849618+00:00", 
"date_fix_committed": "2012-09-27T11:03:37.849618+00:00", 
"date_fix_released": "2012-09-27T11:03:37.849618+00:00", 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1055547", 
"date_created": "2012-09-24T15:01:05.993181+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}, 
{
"date_closed": null, 
"status": "New", 
"bug_target_name": "percona-xtradb-cluster", 
"importance": "Undecided", 
"assignee_link": null, 
"date_triaged": null, 
"bug_id": 1061380, 
"date_in_progress": null, 
"bug_description": "After seeing the following in the error log on the second node:\n\n    121003  7:16:06 [Note] WSREP: Member 0 (joiner) synced with group.\n    121003  7:16:06 [Note] WSREP: Shifting JOINED -> SYNCED (TO: 0)\n    121003  7:16:06 [Note] WSREP: Synchronized with group, ready for connections\n    121003  7:16:06 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.\n    121003  7:17:08 [Note] WSREP: Skipping empty log_xid: COMMIT\n    121003  7:17:08 [Note] WSREP: ignoring DDL failure: 0 ALTER TABLE bigdata_queue_campaigns DISABLE KEYS\n    121003  7:17:08 [Note] WSREP: Skipping empty log_xid: COMMIT\n    121003  7:17:08 [Note] WSREP: ignoring DDL failure: 0 ALTER TABLE bigdata_queue_campaigns ENABLE KEYS\n    \n    ...\n    \n`121003  7:31:33 [ERROR] Slave SQL: Error 'Table 'reportingdb.norep_zonebannertmp_bk' doesn't exist' on query. Default database: 'reportingdb'. Query: 'TRUNCATE TABLE norep_zonebannertmp_bk', Error_code: 1146`\n\n    121003  7:31:33 [Warning] WSREP: RBR event 1 Query apply warning: 1, 1141\n\n`121003  7:31:33 [Warning] WSREP: Ignoring error for TO isolated action: source: 84dcb35c-0ce4-11e2-0800-4568aec9a7f3 version: 2 local: 0 state: APPLYING flags: 65 conn_id: 1106 trx_id: -1 seqnos (l: 1196, g: 1141, s: 1140, d: 1140, ts: 1349224295344525000)`\n\nI cannot login to the first node `mysql -u root -p` hangs. I don't see any interesting in the error log on this node.\n\nI'm using Percona-XtraDB-Cluster-server-5.5.27-23.6.356.rhel5.", 
"target_link": "https://api.launchpad.net/devel/percona-xtradb-cluster", 
"bug_target_display_name": "Percona XtraDB Cluster", 
"related_tasks_collection_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1061380/related_tasks", 
"date_assigned": null, 
"bug_information_type": "Public", 
"bug_title": "One node hangs when another getting a slave SQL error?", 
"title": "Bug #1061380 in Percona XtraDB Cluster: \"One node hangs when another getting a slave SQL error?\"", 
"date_confirmed": null, 
"bug_link": "https://api.launchpad.net/devel/bugs/1061380", 
"date_left_new": null, 
"bug_watch_link": null, 
"web_link": "https://bugs.launchpad.net/percona-xtradb-cluster/+bug/1061380", 
"milestone_link": null, 
"http_etag": "\"94b64d7b9d0517c89505501f953a9d71f1e324d1-568434ed473620b6a990f5c7f78e751518ddfbfc\"", 
"owner_link": "https://api.launchpad.net/devel/~anhquankitty", 
"date_left_closed": null, 
"date_incomplete": null, 
"date_fix_committed": null, 
"date_fix_released": null, 
"self_link": "https://api.launchpad.net/devel/percona-xtradb-cluster/+bug/1061380", 
"date_created": "2012-10-04T04:12:37.747786+00:00", 
"resource_type_link": "https://api.launchpad.net/devel/#bug_task", 
"is_complete": false, 
"bug_tags": []
}
], 
"date_retrieved": "2012-10-17 12:46:45.222154+00:00"
}